{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Preparando-Análise\" data-toc-modified-id=\"Preparando-Análise-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Preparando Análise</a></span></li><li><span><a href=\"#Decision-Trees\" data-toc-modified-id=\"Decision-Trees-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Decision Trees</a></span><ul class=\"toc-item\"><li><span><a href=\"#Dados-de-Treino\" data-toc-modified-id=\"Dados-de-Treino-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Dados de Treino</a></span></li><li><span><a href=\"#Tunando-Hiperparâmetros\" data-toc-modified-id=\"Tunando-Hiperparâmetros-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Tunando Hiperparâmetros</a></span></li><li><span><a href=\"#Dados-de-Teste\" data-toc-modified-id=\"Dados-de-Teste-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Dados de Teste</a></span></li><li><span><a href=\"#Shifting-Images\" data-toc-modified-id=\"Shifting-Images-2.4\"><span class=\"toc-item-num\">2.4&nbsp;&nbsp;</span>Shifting Images</a></span></li></ul></li><li><span><a href=\"#SGD-Classifier\" data-toc-modified-id=\"SGD-Classifier-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>SGD Classifier</a></span><ul class=\"toc-item\"><li><span><a href=\"#Dados-de-Treino\" data-toc-modified-id=\"Dados-de-Treino-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Dados de Treino</a></span></li><li><span><a href=\"#Tunando-Hiperparâmetros\" data-toc-modified-id=\"Tunando-Hiperparâmetros-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Tunando Hiperparâmetros</a></span></li><li><span><a href=\"#Dados-de-Teste\" data-toc-modified-id=\"Dados-de-Teste-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>Dados de Teste</a></span></li><li><span><a href=\"#Shifting-Images\" data-toc-modified-id=\"Shifting-Images-3.4\"><span class=\"toc-item-num\">3.4&nbsp;&nbsp;</span>Shifting Images</a></span></li></ul></li><li><span><a href=\"#Random-Forest\" data-toc-modified-id=\"Random-Forest-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Random Forest</a></span><ul class=\"toc-item\"><li><span><a href=\"#Dados-de-Treino\" data-toc-modified-id=\"Dados-de-Treino-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Dados de Treino</a></span></li><li><span><a href=\"#Tunando-Hiperparâmetros\" data-toc-modified-id=\"Tunando-Hiperparâmetros-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Tunando Hiperparâmetros</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este notebook tem por objetivo comparar diferentes algoritmos utilizados em problemas de classificação através do dataset MNIST. Com isso, será possível avaliar a performance de cada um deles, avaliando, analisando e identificando importantes parâmetros como tempo de processamento e complexidade dos modelos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparando Análise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-06T19:37:53.898557Z",
     "start_time": "2019-01-06T19:37:52.532494Z"
    }
   },
   "outputs": [],
   "source": [
    "# Importando bibliotecas para análise e preparação dos dados\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import fetch_mldata\n",
    "from random import randint\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.ndimage.interpolation import shift\n",
    "\n",
    "# Importando bibliotecas para treinamento de algoritmos\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "# Importando bibliotecas para avaliação de performance\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-06T22:41:57.117041Z",
     "start_time": "2019-01-06T22:41:56.586482Z"
    }
   },
   "outputs": [],
   "source": [
    "# Definindo funções e variáveis\n",
    "\n",
    "def create_dataset():\n",
    "    \"\"\"\n",
    "    Função para criação (primeira vez) de um dataset vazio para armazenar acurácias\n",
    "    \"\"\"\n",
    "    dataset_cols = ['acc_train', 'acc_train_cv', 'acc_train_scaled', 'acc_test'\n",
    "                   ,'acc_test_scaled', 'acc_test_grid', 'acc_test_shifted']\n",
    "    dict_accs = {}\n",
    "    dataset_accs = pd.DataFrame({})\n",
    "    for col in dataset_cols:\n",
    "        dataset_accs[col] = []\n",
    "    return dataset_accs, dict_accs\n",
    "\n",
    "def load_dataset():\n",
    "    \"\"\"\n",
    "    Função responsável por ler dataset já criado (a partir da primeira vez)\n",
    "    \n",
    "    Output:\n",
    "        DataFrame\n",
    "    \"\"\"\n",
    "    path = 'C:/Users/thiagoPanini/data-science-repos/coursera-stanford/'\n",
    "    root = 'machine-learning/notations/semana-5/python-programs/'\n",
    "    file_name = 'dataset_accs.csv'\n",
    "    \n",
    "    return pd.read_csv(path+root+file_name)\n",
    "\n",
    "def save_dataset(df):\n",
    "    \"\"\"\n",
    "    Função responsável por salvar o dataset a partir da primeira análise\n",
    "    \n",
    "    Input:\n",
    "        DataFrame a ser salvo\n",
    "    \"\"\"\n",
    "    df.to_csv('dataset_accs.csv', index=False)\n",
    "    \n",
    "def display_scores(scores):\n",
    "    \"\"\"\n",
    "    Função para mostrar scores da validação cruzada\n",
    "    \"\"\"\n",
    "    print(f'Scores: {scores}')\n",
    "    print(f'Média: {scores.mean():.4f}')\n",
    "    print(f'Desvio Padrão: {scores.std():.4f}')\n",
    "    \n",
    "def prepare_mnist():\n",
    "    \"\"\"\n",
    "    Função responsável por importar e realizar todos os procedimentos preparatórios\n",
    "    no dataset MNIST.\n",
    "    \n",
    "    Output: X_train, y_train, X_test, y_test\n",
    "    \"\"\"\n",
    "    \n",
    "    # Download das bibliotecas necessárias\n",
    "    from sklearn.datasets import fetch_mldata\n",
    "    import numpy as np\n",
    "    \n",
    "    # Download e separação do dataset\n",
    "    mnist = fetch_mldata('MNIST original')\n",
    "    X, y = mnist['data'], mnist['target']\n",
    "    \n",
    "    # Separando dados de treino e de teste\n",
    "    X_train, y_train, X_test, y_test = X[:60000], y[:60000], X[60000:], y[60000:]\n",
    "    \n",
    "    # Embaralhando dados\n",
    "    shuffle_index = np.random.permutation(X_train.shape[0])\n",
    "    X_train, y_train = X_train[shuffle_index], y_train[shuffle_index]\n",
    "    \n",
    "    # Retornando dados\n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "def data_scaled(X_train, X_test):\n",
    "    \"\"\"\n",
    "    Função responsável por aplicar padronização no conjunto de dados\n",
    "    \n",
    "    Input: X_train, X_test -> dados brutos\n",
    "    Output: X_train_scaled, X_test_scaled -> dados padronizados\n",
    "    \"\"\"\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.fit_transform(X_test)\n",
    "    \n",
    "    return X_train_scaled, X_test_scaled\n",
    "\n",
    "def shift_image(image, dx, dy):\n",
    "    \"\"\"\n",
    "    Função responsável por deslocar imagens do dataset em determinada direção\n",
    "    \n",
    "    Input: \n",
    "        image (registro do dataset X_train)\n",
    "        dx, dy (deslocamentos em x e y)\n",
    "    Output:\n",
    "        shifted_image (imagem deslocada)\n",
    "    \"\"\"\n",
    "    image = image.reshape((28, 28))\n",
    "    shift_image = shift(image, [dy, dx], cval=0, mode='constant')\n",
    "    return shift_image.reshape([-1])\n",
    "\n",
    "def augment_data(X_train, y_train):\n",
    "    \"\"\"\n",
    "    Função para deslocar imagens do dataset em todas as direções\n",
    "    \n",
    "    Input:\n",
    "        X_train, y_train (dados brutos de entrada)\n",
    "    Output:\n",
    "        X_train_augmented, y_train_augmented (dados deslocados e embaralhados em formato de array)\n",
    "    \"\"\"\n",
    "    X_train_augmented = [image for image in X_train]\n",
    "    y_train_augmented = [label for label in y_train]\n",
    "    \n",
    "    for dx, dy in ((1, 0), (-1, 0), (0, 1), (0, -1)):\n",
    "        for image, label in zip(X_train, y_train):\n",
    "            X_train_augmented.append(shift_image(image, dx, dy))\n",
    "            y_train_augmented.append(label)\n",
    "    \n",
    "    X_train_augmented = np.array(X_train_augmented)\n",
    "    y_train_augmented = np.array(y_train_augmented)\n",
    "    \n",
    "    shuffle_idx = np.random.permutation(len(X_train_augmented))\n",
    "    X_train_augmented = X_train_augmented[shuffle_idx]\n",
    "    y_train_augmented = y_train_augmented[shuffle_idx]\n",
    "    \n",
    "    return X_train_augmented, y_train_augmented\n",
    "\n",
    "def plot_mnist(X, cmap='binary'):\n",
    "    \"\"\"\n",
    "    Função para plotar graficamente dígitos aleatórios do dataset MNIST\n",
    "    \"\"\"\n",
    "    some_digit = X[randint(0, X.shape[0])]\n",
    "    some_digit_reshaped = some_digit.reshape(28, 28)\n",
    "    plt.imshow(some_digit_reshaped, cmap=cmap, interpolation='nearest')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "def plot_data_augmented(X):\n",
    "    \"\"\"\n",
    "    Função responsável por plotar graficamente imagens deslocadas do dataset\n",
    "    \"\"\"\n",
    "    image = X[randint(0, X.shape[0])]\n",
    "    shifted_image_down = shift_image(image, 0, 5)\n",
    "    shifted_image_left = shift_image(image, -5, 0)\n",
    "    shifted_image_up = shift_image(image, 0, -5)\n",
    "    shifted_image_right = shift_image(image, 5, 0)\n",
    "    \n",
    "    # Criando figure e plotando gráficos\n",
    "    plt.figure(figsize=(12, 3))\n",
    "    plt.subplot(151)\n",
    "    plt.title('Original', fontsize=14)\n",
    "    plt.imshow(image.reshape(28, 28), interpolation=\"nearest\", cmap=\"Greys\")\n",
    "    plt.subplot(152)\n",
    "    plt.title('Shifted down', fontsize=14)\n",
    "    plt.imshow(shifted_image_down.reshape(28, 28), interpolation=\"nearest\", cmap=\"Greys\")\n",
    "    plt.subplot(153)\n",
    "    plt.title('Shifted left', fontsize=14)\n",
    "    plt.imshow(shifted_image_left.reshape(28, 28), interpolation=\"nearest\", cmap=\"Greys\")\n",
    "    plt.subplot(154)\n",
    "    plt.title('Shifted up', fontsize=14)\n",
    "    plt.imshow(shifted_image_up.reshape(28, 28), interpolation=\"nearest\", cmap=\"Greys\")\n",
    "    plt.subplot(155)\n",
    "    plt.title('Shifted right', fontsize=14)\n",
    "    plt.imshow(shifted_image_right.reshape(28, 28), interpolation=\"nearest\", cmap=\"Greys\")\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explorando Dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com a definição das funções e importação das bibliotecas necessárias, os códigos a seguir terão como objetivo mostrar um pouco mais sobre o dataset MNIST, como o download e plotagens dos dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-06T19:37:58.514474Z",
     "start_time": "2019-01-06T19:37:58.409685Z"
    }
   },
   "outputs": [],
   "source": [
    "# Realizando o download e a preparação dos dados\n",
    "X_train, y_train, X_test, y_test = prepare_mnist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-06T19:37:59.318943Z",
     "start_time": "2019-01-06T19:37:59.186577Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2083bc837f0>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotando dados\n",
    "plot_mnist(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-06T19:38:00.631276Z",
     "start_time": "2019-01-06T19:38:00.621306Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensões de X_train: (60000, 784)\n",
      "Dimensões de y_train: (60000,)\n",
      "\n",
      "Dimensões de X_test: (10000, 784)\n",
      "Dimensões de y_test: (10000,)\n"
     ]
    }
   ],
   "source": [
    "# Verificando dimensões\n",
    "print(f'Dimensões de X_train: {X_train.shape}')\n",
    "print(f'Dimensões de y_train: {y_train.shape}')\n",
    "print()\n",
    "print(f'Dimensões de X_test: {X_test.shape}')\n",
    "print(f'Dimensões de y_test: {y_test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trata-se de um dataset com imagens de dimensões 28x28 (totalizando 784 features). Os dados de treino possuem 60000 registros, enquanto os dados de teste são compostos por 10000 registros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-06T19:38:02.385569Z",
     "start_time": "2019-01-06T19:38:02.270816Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2083bc40d30>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotagem diferenciada\n",
    "plot_mnist(X_train, cmap='viridis')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O primeiro algoritmo utilizado será o ```DecisionTrees```: [Doc](https://scikit-learn.org/stable/modules/tree.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-06T19:38:04.445244Z",
     "start_time": "2019-01-06T19:38:04.433278Z"
    }
   },
   "outputs": [],
   "source": [
    "# Como é a primeira análise, temos que criar um dataset vazio\n",
    "dataset_accs, dict_accs_tree = create_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-06T19:38:06.082485Z",
     "start_time": "2019-01-06T19:38:06.065534Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acc_train</th>\n",
       "      <th>acc_train_cv</th>\n",
       "      <th>acc_train_scaled</th>\n",
       "      <th>acc_test</th>\n",
       "      <th>acc_test_scaled</th>\n",
       "      <th>acc_test_grid</th>\n",
       "      <th>acc_test_shifted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [acc_train, acc_train_cv, acc_train_scaled, acc_test, acc_test_scaled, acc_test_grid, acc_test_shifted]\n",
       "Index: []"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dataset criado\n",
    "dataset_accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-06T19:38:06.929471Z",
     "start_time": "2019-01-06T19:38:06.923455Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dicionário criado para armazenar acurácias\n",
    "dict_accs_tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dados de Treino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-06T19:38:42.884881Z",
     "start_time": "2019-01-06T19:38:08.254471Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia do Decision Trees com dados de treino: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# Criando classificador\n",
    "dtree = DecisionTreeClassifier()\n",
    "\n",
    "# Treinando modelo (sem transformações)\n",
    "dtree.fit(X_train, y_train)\n",
    "\n",
    "# Avaliando acurácia\n",
    "train_pred = dtree.predict(X_train)\n",
    "train_acc = accuracy_score(y_train, train_pred)\n",
    "print(f'Acurácia do Decision Trees com dados de treino: {train_acc:.4f}')\n",
    "\n",
    "# Salvando dados\n",
    "dict_accs_tree['acc_train'] = round(train_acc, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Será um tremendo overfitting? Sempre é preciso desconfiar de uma grande acurácia nos dados de treino. Para verificar se realmente há a presença de overfitting, devemos aplicar o ```cross_validation```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-06T19:39:44.435319Z",
     "start_time": "2019-01-06T19:38:42.887944Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores: [0.86182763 0.86124306 0.86192929]\n",
      "Média: 0.8617\n",
      "Desvio Padrão: 0.0003\n"
     ]
    }
   ],
   "source": [
    "# Aplicando validação cruzada\n",
    "tree_scores = cross_val_score(dtree, X_train, y_train,\n",
    "                             cv=3, scoring='accuracy')\n",
    "display_scores(tree_scores)\n",
    "\n",
    "# Salvando dados\n",
    "dict_accs_tree['acc_train_cv'] = round(tree_scores.mean(), 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A diminuição da acurácia através da aplicação do ```cross validation``` mostra que realmente o modelo ```DecisionTrees``` é muito sensível ao overfitting.\n",
    "\n",
    "Na tentativa de melhorar essa performance, vamos aplicar um processo de ```padronização``` nos dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-06T19:43:35.351608Z",
     "start_time": "2019-01-06T19:41:56.261408Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thiagoPanini\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:475: DataConversionWarning: Data with input dtype uint8 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores: [0.85972805 0.86089304 0.8606791 ]\n",
      "Média: 0.8604\n",
      "Desvio Padrão: 0.0005\n"
     ]
    }
   ],
   "source": [
    "# Padronizando dados\n",
    "X_train_scaled, X_test_scaled = data_scaled(X_train, X_test)\n",
    "\n",
    "# Realizando um novo treinamento\n",
    "dtree.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Verificando nova acurácia com cross validation\n",
    "tree_scores_scaled = cross_val_score(dtree, X_train_scaled, y_train,\n",
    "                                    cv=3, scoring='accuracy')\n",
    "\n",
    "# Comunicando resultados\n",
    "display_scores(tree_scores_scaled)\n",
    "\n",
    "# Salvando dados\n",
    "dict_accs_tree['acc_train_scaled'] = round(tree_scores_scaled.mean(), 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Não houve alterações significativas. Inclusive foi identificado um decréscimo mínimo na acurácia.\n",
    "\n",
    "Assim, é possível concluir que o modelo ```DecisionTrees``` não é sensível a padronização dos dados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tunando Hiperparâmetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-06T20:34:37.188625Z",
     "start_time": "2019-01-06T19:43:43.858248Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 72 candidates, totalling 216 fits\n",
      "[CV] criterion=gini, max_depth=None, min_samples_leaf=1, min_samples_split=2 \n",
      "[CV]  criterion=gini, max_depth=None, min_samples_leaf=1, min_samples_split=2, score=0.8591781643671266, total=  21.0s\n",
      "[CV] criterion=gini, max_depth=None, min_samples_leaf=1, min_samples_split=2 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   21.1s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  criterion=gini, max_depth=None, min_samples_leaf=1, min_samples_split=2, score=0.8609430471523576, total=  19.5s\n",
      "[CV] criterion=gini, max_depth=None, min_samples_leaf=1, min_samples_split=2 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:   40.9s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  criterion=gini, max_depth=None, min_samples_leaf=1, min_samples_split=2, score=0.8630794619192879, total=  21.2s\n",
      "[CV] criterion=gini, max_depth=None, min_samples_leaf=1, min_samples_split=10 \n",
      "[CV]  criterion=gini, max_depth=None, min_samples_leaf=1, min_samples_split=10, score=0.860877824435113, total=  21.0s\n",
      "[CV] criterion=gini, max_depth=None, min_samples_leaf=1, min_samples_split=10 \n",
      "[CV]  criterion=gini, max_depth=None, min_samples_leaf=1, min_samples_split=10, score=0.8580429021451073, total=  19.3s\n",
      "[CV] criterion=gini, max_depth=None, min_samples_leaf=1, min_samples_split=10 \n",
      "[CV]  criterion=gini, max_depth=None, min_samples_leaf=1, min_samples_split=10, score=0.8612291843776566, total=  21.0s\n",
      "[CV] criterion=gini, max_depth=None, min_samples_leaf=1, min_samples_split=20 \n",
      "[CV]  criterion=gini, max_depth=None, min_samples_leaf=1, min_samples_split=20, score=0.8565286942611477, total=  20.9s\n",
      "[CV] criterion=gini, max_depth=None, min_samples_leaf=1, min_samples_split=20 \n",
      "[CV]  criterion=gini, max_depth=None, min_samples_leaf=1, min_samples_split=20, score=0.8541927096354818, total=  19.1s\n",
      "[CV] criterion=gini, max_depth=None, min_samples_leaf=1, min_samples_split=20 \n",
      "[CV]  criterion=gini, max_depth=None, min_samples_leaf=1, min_samples_split=20, score=0.8566284942741411, total=  21.6s\n",
      "[CV] criterion=gini, max_depth=None, min_samples_leaf=5, min_samples_split=2 \n",
      "[CV]  criterion=gini, max_depth=None, min_samples_leaf=5, min_samples_split=2, score=0.862127574485103, total=  17.2s\n",
      "[CV] criterion=gini, max_depth=None, min_samples_leaf=5, min_samples_split=2 \n",
      "[CV]  criterion=gini, max_depth=None, min_samples_leaf=5, min_samples_split=2, score=0.8581929096454822, total=  17.4s\n",
      "[CV] criterion=gini, max_depth=None, min_samples_leaf=5, min_samples_split=2 \n",
      "[CV]  criterion=gini, max_depth=None, min_samples_leaf=5, min_samples_split=2, score=0.8670300545081763, total=  18.0s\n",
      "[CV] criterion=gini, max_depth=None, min_samples_leaf=5, min_samples_split=10 \n",
      "[CV]  criterion=gini, max_depth=None, min_samples_leaf=5, min_samples_split=10, score=0.8617776444711058, total=  18.7s\n",
      "[CV] criterion=gini, max_depth=None, min_samples_leaf=5, min_samples_split=10 \n",
      "[CV]  criterion=gini, max_depth=None, min_samples_leaf=5, min_samples_split=10, score=0.8590929546477324, total=  16.9s\n",
      "[CV] criterion=gini, max_depth=None, min_samples_leaf=5, min_samples_split=10 \n",
      "[CV]  criterion=gini, max_depth=None, min_samples_leaf=5, min_samples_split=10, score=0.8660299044856729, total=  18.1s\n",
      "[CV] criterion=gini, max_depth=None, min_samples_leaf=5, min_samples_split=20 \n",
      "[CV]  criterion=gini, max_depth=None, min_samples_leaf=5, min_samples_split=20, score=0.8607778444311138, total=  17.8s\n",
      "[CV] criterion=gini, max_depth=None, min_samples_leaf=5, min_samples_split=20 \n",
      "[CV]  criterion=gini, max_depth=None, min_samples_leaf=5, min_samples_split=20, score=0.8565928296414821, total=  17.1s\n",
      "[CV] criterion=gini, max_depth=None, min_samples_leaf=5, min_samples_split=20 \n",
      "[CV]  criterion=gini, max_depth=None, min_samples_leaf=5, min_samples_split=20, score=0.8621293193979097, total=  18.1s\n",
      "[CV] criterion=gini, max_depth=None, min_samples_leaf=10, min_samples_split=2 \n",
      "[CV]  criterion=gini, max_depth=None, min_samples_leaf=10, min_samples_split=2, score=0.8582783443311338, total=  16.0s\n",
      "[CV] criterion=gini, max_depth=None, min_samples_leaf=10, min_samples_split=2 \n",
      "[CV]  criterion=gini, max_depth=None, min_samples_leaf=10, min_samples_split=2, score=0.855892794639732, total=  15.7s\n",
      "[CV] criterion=gini, max_depth=None, min_samples_leaf=10, min_samples_split=2 \n",
      "[CV]  criterion=gini, max_depth=None, min_samples_leaf=10, min_samples_split=2, score=0.8633294994249138, total=  15.6s\n",
      "[CV] criterion=gini, max_depth=None, min_samples_leaf=10, min_samples_split=10 \n",
      "[CV]  criterion=gini, max_depth=None, min_samples_leaf=10, min_samples_split=10, score=0.8583283343331334, total=  16.5s\n",
      "[CV] criterion=gini, max_depth=None, min_samples_leaf=10, min_samples_split=10 \n",
      "[CV]  criterion=gini, max_depth=None, min_samples_leaf=10, min_samples_split=10, score=0.856392819640982, total=  16.4s\n",
      "[CV] criterion=gini, max_depth=None, min_samples_leaf=10, min_samples_split=10 \n",
      "[CV]  criterion=gini, max_depth=None, min_samples_leaf=10, min_samples_split=10, score=0.8628794319147872, total=  17.6s\n",
      "[CV] criterion=gini, max_depth=None, min_samples_leaf=10, min_samples_split=20 \n",
      "[CV]  criterion=gini, max_depth=None, min_samples_leaf=10, min_samples_split=20, score=0.8583283343331334, total=  18.0s\n",
      "[CV] criterion=gini, max_depth=None, min_samples_leaf=10, min_samples_split=20 \n",
      "[CV]  criterion=gini, max_depth=None, min_samples_leaf=10, min_samples_split=20, score=0.855942797139857, total=  17.8s\n",
      "[CV] criterion=gini, max_depth=None, min_samples_leaf=10, min_samples_split=20 \n",
      "[CV]  criterion=gini, max_depth=None, min_samples_leaf=10, min_samples_split=20, score=0.862529379406911, total=  17.0s\n",
      "[CV] criterion=gini, max_depth=5, min_samples_leaf=1, min_samples_split=2 \n",
      "[CV]  criterion=gini, max_depth=5, min_samples_leaf=1, min_samples_split=2, score=0.6662667466506699, total=   6.1s\n",
      "[CV] criterion=gini, max_depth=5, min_samples_leaf=1, min_samples_split=2 \n",
      "[CV]  criterion=gini, max_depth=5, min_samples_leaf=1, min_samples_split=2, score=0.6806340317015851, total=   5.8s\n",
      "[CV] criterion=gini, max_depth=5, min_samples_leaf=1, min_samples_split=2 \n",
      "[CV]  criterion=gini, max_depth=5, min_samples_leaf=1, min_samples_split=2, score=0.6674501175176276, total=   6.1s\n",
      "[CV] criterion=gini, max_depth=5, min_samples_leaf=1, min_samples_split=10 \n",
      "[CV]  criterion=gini, max_depth=5, min_samples_leaf=1, min_samples_split=10, score=0.6662667466506699, total=   6.0s\n",
      "[CV] criterion=gini, max_depth=5, min_samples_leaf=1, min_samples_split=10 \n",
      "[CV]  criterion=gini, max_depth=5, min_samples_leaf=1, min_samples_split=10, score=0.6806340317015851, total=   6.0s\n",
      "[CV] criterion=gini, max_depth=5, min_samples_leaf=1, min_samples_split=10 \n",
      "[CV]  criterion=gini, max_depth=5, min_samples_leaf=1, min_samples_split=10, score=0.6674501175176276, total=   6.2s\n",
      "[CV] criterion=gini, max_depth=5, min_samples_leaf=1, min_samples_split=20 \n",
      "[CV]  criterion=gini, max_depth=5, min_samples_leaf=1, min_samples_split=20, score=0.6662667466506699, total=   6.0s\n",
      "[CV] criterion=gini, max_depth=5, min_samples_leaf=1, min_samples_split=20 \n",
      "[CV]  criterion=gini, max_depth=5, min_samples_leaf=1, min_samples_split=20, score=0.6806340317015851, total=   6.6s\n",
      "[CV] criterion=gini, max_depth=5, min_samples_leaf=1, min_samples_split=20 \n",
      "[CV]  criterion=gini, max_depth=5, min_samples_leaf=1, min_samples_split=20, score=0.6674501175176276, total=   6.9s\n",
      "[CV] criterion=gini, max_depth=5, min_samples_leaf=5, min_samples_split=2 \n",
      "[CV]  criterion=gini, max_depth=5, min_samples_leaf=5, min_samples_split=2, score=0.6662667466506699, total=   6.6s\n",
      "[CV] criterion=gini, max_depth=5, min_samples_leaf=5, min_samples_split=2 \n",
      "[CV]  criterion=gini, max_depth=5, min_samples_leaf=5, min_samples_split=2, score=0.6806340317015851, total=   6.5s\n",
      "[CV] criterion=gini, max_depth=5, min_samples_leaf=5, min_samples_split=2 \n",
      "[CV]  criterion=gini, max_depth=5, min_samples_leaf=5, min_samples_split=2, score=0.6674501175176276, total=   6.0s\n",
      "[CV] criterion=gini, max_depth=5, min_samples_leaf=5, min_samples_split=10 \n",
      "[CV]  criterion=gini, max_depth=5, min_samples_leaf=5, min_samples_split=10, score=0.6662667466506699, total=   6.1s\n",
      "[CV] criterion=gini, max_depth=5, min_samples_leaf=5, min_samples_split=10 \n",
      "[CV]  criterion=gini, max_depth=5, min_samples_leaf=5, min_samples_split=10, score=0.6806340317015851, total=   6.1s\n",
      "[CV] criterion=gini, max_depth=5, min_samples_leaf=5, min_samples_split=10 \n",
      "[CV]  criterion=gini, max_depth=5, min_samples_leaf=5, min_samples_split=10, score=0.6674501175176276, total=   6.1s\n",
      "[CV] criterion=gini, max_depth=5, min_samples_leaf=5, min_samples_split=20 \n",
      "[CV]  criterion=gini, max_depth=5, min_samples_leaf=5, min_samples_split=20, score=0.6662667466506699, total=   6.1s\n",
      "[CV] criterion=gini, max_depth=5, min_samples_leaf=5, min_samples_split=20 \n",
      "[CV]  criterion=gini, max_depth=5, min_samples_leaf=5, min_samples_split=20, score=0.6805340267013351, total=   5.8s\n",
      "[CV] criterion=gini, max_depth=5, min_samples_leaf=5, min_samples_split=20 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  criterion=gini, max_depth=5, min_samples_leaf=5, min_samples_split=20, score=0.6674501175176276, total=   6.6s\n",
      "[CV] criterion=gini, max_depth=5, min_samples_leaf=10, min_samples_split=2 \n",
      "[CV]  criterion=gini, max_depth=5, min_samples_leaf=10, min_samples_split=2, score=0.6662667466506699, total=   6.4s\n",
      "[CV] criterion=gini, max_depth=5, min_samples_leaf=10, min_samples_split=2 \n",
      "[CV]  criterion=gini, max_depth=5, min_samples_leaf=10, min_samples_split=2, score=0.6806340317015851, total=   6.0s\n",
      "[CV] criterion=gini, max_depth=5, min_samples_leaf=10, min_samples_split=2 \n",
      "[CV]  criterion=gini, max_depth=5, min_samples_leaf=10, min_samples_split=2, score=0.6674501175176276, total=   6.3s\n",
      "[CV] criterion=gini, max_depth=5, min_samples_leaf=10, min_samples_split=10 \n",
      "[CV]  criterion=gini, max_depth=5, min_samples_leaf=10, min_samples_split=10, score=0.6662667466506699, total=   6.2s\n",
      "[CV] criterion=gini, max_depth=5, min_samples_leaf=10, min_samples_split=10 \n",
      "[CV]  criterion=gini, max_depth=5, min_samples_leaf=10, min_samples_split=10, score=0.6806340317015851, total=   6.0s\n",
      "[CV] criterion=gini, max_depth=5, min_samples_leaf=10, min_samples_split=10 \n",
      "[CV]  criterion=gini, max_depth=5, min_samples_leaf=10, min_samples_split=10, score=0.6674501175176276, total=   6.4s\n",
      "[CV] criterion=gini, max_depth=5, min_samples_leaf=10, min_samples_split=20 \n",
      "[CV]  criterion=gini, max_depth=5, min_samples_leaf=10, min_samples_split=20, score=0.6662667466506699, total=   6.0s\n",
      "[CV] criterion=gini, max_depth=5, min_samples_leaf=10, min_samples_split=20 \n",
      "[CV]  criterion=gini, max_depth=5, min_samples_leaf=10, min_samples_split=20, score=0.6805340267013351, total=   6.3s\n",
      "[CV] criterion=gini, max_depth=5, min_samples_leaf=10, min_samples_split=20 \n",
      "[CV]  criterion=gini, max_depth=5, min_samples_leaf=10, min_samples_split=20, score=0.6674501175176276, total=   6.1s\n",
      "[CV] criterion=gini, max_depth=10, min_samples_leaf=1, min_samples_split=2 \n",
      "[CV]  criterion=gini, max_depth=10, min_samples_leaf=1, min_samples_split=2, score=0.8450809838032394, total=  10.9s\n",
      "[CV] criterion=gini, max_depth=10, min_samples_leaf=1, min_samples_split=2 \n",
      "[CV]  criterion=gini, max_depth=10, min_samples_leaf=1, min_samples_split=2, score=0.847942397119856, total=  10.9s\n",
      "[CV] criterion=gini, max_depth=10, min_samples_leaf=1, min_samples_split=2 \n",
      "[CV]  criterion=gini, max_depth=10, min_samples_leaf=1, min_samples_split=2, score=0.8523778566785017, total=  10.9s\n",
      "[CV] criterion=gini, max_depth=10, min_samples_leaf=1, min_samples_split=10 \n",
      "[CV]  criterion=gini, max_depth=10, min_samples_leaf=1, min_samples_split=10, score=0.8437312537492502, total=  11.0s\n",
      "[CV] criterion=gini, max_depth=10, min_samples_leaf=1, min_samples_split=10 \n",
      "[CV]  criterion=gini, max_depth=10, min_samples_leaf=1, min_samples_split=10, score=0.8470923546177309, total=  10.7s\n",
      "[CV] criterion=gini, max_depth=10, min_samples_leaf=1, min_samples_split=10 \n",
      "[CV]  criterion=gini, max_depth=10, min_samples_leaf=1, min_samples_split=10, score=0.8525278791818773, total=  10.9s\n",
      "[CV] criterion=gini, max_depth=10, min_samples_leaf=1, min_samples_split=20 \n",
      "[CV]  criterion=gini, max_depth=10, min_samples_leaf=1, min_samples_split=20, score=0.8415316936612678, total=  10.8s\n",
      "[CV] criterion=gini, max_depth=10, min_samples_leaf=1, min_samples_split=20 \n",
      "[CV]  criterion=gini, max_depth=10, min_samples_leaf=1, min_samples_split=20, score=0.8453922696134807, total=  10.9s\n",
      "[CV] criterion=gini, max_depth=10, min_samples_leaf=1, min_samples_split=20 \n",
      "[CV]  criterion=gini, max_depth=10, min_samples_leaf=1, min_samples_split=20, score=0.8498274741211181, total=  10.6s\n",
      "[CV] criterion=gini, max_depth=10, min_samples_leaf=5, min_samples_split=2 \n",
      "[CV]  criterion=gini, max_depth=10, min_samples_leaf=5, min_samples_split=2, score=0.8437812437512497, total=  10.6s\n",
      "[CV] criterion=gini, max_depth=10, min_samples_leaf=5, min_samples_split=2 \n",
      "[CV]  criterion=gini, max_depth=10, min_samples_leaf=5, min_samples_split=2, score=0.847792389619481, total=  10.6s\n",
      "[CV] criterion=gini, max_depth=10, min_samples_leaf=5, min_samples_split=2 \n",
      "[CV]  criterion=gini, max_depth=10, min_samples_leaf=5, min_samples_split=2, score=0.8507776166424964, total=  10.9s\n",
      "[CV] criterion=gini, max_depth=10, min_samples_leaf=5, min_samples_split=10 \n",
      "[CV]  criterion=gini, max_depth=10, min_samples_leaf=5, min_samples_split=10, score=0.8437312537492502, total=  11.0s\n",
      "[CV] criterion=gini, max_depth=10, min_samples_leaf=5, min_samples_split=10 \n",
      "[CV]  criterion=gini, max_depth=10, min_samples_leaf=5, min_samples_split=10, score=0.847842392119606, total=  10.7s\n",
      "[CV] criterion=gini, max_depth=10, min_samples_leaf=5, min_samples_split=10 \n",
      "[CV]  criterion=gini, max_depth=10, min_samples_leaf=5, min_samples_split=10, score=0.8510776616492474, total=  10.8s\n",
      "[CV] criterion=gini, max_depth=10, min_samples_leaf=5, min_samples_split=20 \n",
      "[CV]  criterion=gini, max_depth=10, min_samples_leaf=5, min_samples_split=20, score=0.8419316136772645, total=  10.6s\n",
      "[CV] criterion=gini, max_depth=10, min_samples_leaf=5, min_samples_split=20 \n",
      "[CV]  criterion=gini, max_depth=10, min_samples_leaf=5, min_samples_split=20, score=0.8465923296164808, total=  11.0s\n",
      "[CV] criterion=gini, max_depth=10, min_samples_leaf=5, min_samples_split=20 \n",
      "[CV]  criterion=gini, max_depth=10, min_samples_leaf=5, min_samples_split=20, score=0.8492773916087413, total=  10.6s\n",
      "[CV] criterion=gini, max_depth=10, min_samples_leaf=10, min_samples_split=2 \n",
      "[CV]  criterion=gini, max_depth=10, min_samples_leaf=10, min_samples_split=2, score=0.8402819436112777, total=  10.7s\n",
      "[CV] criterion=gini, max_depth=10, min_samples_leaf=10, min_samples_split=2 \n",
      "[CV]  criterion=gini, max_depth=10, min_samples_leaf=10, min_samples_split=2, score=0.8445922296114806, total=  10.7s\n",
      "[CV] criterion=gini, max_depth=10, min_samples_leaf=10, min_samples_split=2 \n",
      "[CV]  criterion=gini, max_depth=10, min_samples_leaf=10, min_samples_split=2, score=0.8484772715907386, total=  10.6s\n",
      "[CV] criterion=gini, max_depth=10, min_samples_leaf=10, min_samples_split=10 \n",
      "[CV]  criterion=gini, max_depth=10, min_samples_leaf=10, min_samples_split=10, score=0.8406818636272746, total=  10.9s\n",
      "[CV] criterion=gini, max_depth=10, min_samples_leaf=10, min_samples_split=10 \n",
      "[CV]  criterion=gini, max_depth=10, min_samples_leaf=10, min_samples_split=10, score=0.8446422321116056, total=  10.6s\n",
      "[CV] criterion=gini, max_depth=10, min_samples_leaf=10, min_samples_split=10 \n",
      "[CV]  criterion=gini, max_depth=10, min_samples_leaf=10, min_samples_split=10, score=0.8480772115817372, total=  10.6s\n",
      "[CV] criterion=gini, max_depth=10, min_samples_leaf=10, min_samples_split=20 \n",
      "[CV]  criterion=gini, max_depth=10, min_samples_leaf=10, min_samples_split=20, score=0.840381923615277, total=  10.6s\n",
      "[CV] criterion=gini, max_depth=10, min_samples_leaf=10, min_samples_split=20 \n",
      "[CV]  criterion=gini, max_depth=10, min_samples_leaf=10, min_samples_split=20, score=0.8446922346117306, total=  10.4s\n",
      "[CV] criterion=gini, max_depth=10, min_samples_leaf=10, min_samples_split=20 \n",
      "[CV]  criterion=gini, max_depth=10, min_samples_leaf=10, min_samples_split=20, score=0.848277241586238, total=  10.6s\n",
      "[CV] criterion=gini, max_depth=15, min_samples_leaf=1, min_samples_split=2 \n",
      "[CV]  criterion=gini, max_depth=15, min_samples_leaf=1, min_samples_split=2, score=0.8638272345530894, total=  14.0s\n",
      "[CV] criterion=gini, max_depth=15, min_samples_leaf=1, min_samples_split=2 \n",
      "[CV]  criterion=gini, max_depth=15, min_samples_leaf=1, min_samples_split=2, score=0.8631431571578579, total=  14.5s\n",
      "[CV] criterion=gini, max_depth=15, min_samples_leaf=1, min_samples_split=2 \n",
      "[CV]  criterion=gini, max_depth=15, min_samples_leaf=1, min_samples_split=2, score=0.8673300995149272, total=  15.1s\n",
      "[CV] criterion=gini, max_depth=15, min_samples_leaf=1, min_samples_split=10 \n",
      "[CV]  criterion=gini, max_depth=15, min_samples_leaf=1, min_samples_split=10, score=0.861127774445111, total=  14.6s\n",
      "[CV] criterion=gini, max_depth=15, min_samples_leaf=1, min_samples_split=10 \n",
      "[CV]  criterion=gini, max_depth=15, min_samples_leaf=1, min_samples_split=10, score=0.8622431121556078, total=  14.4s\n",
      "[CV] criterion=gini, max_depth=15, min_samples_leaf=1, min_samples_split=10 \n",
      "[CV]  criterion=gini, max_depth=15, min_samples_leaf=1, min_samples_split=10, score=0.8646797019552933, total=  14.2s\n",
      "[CV] criterion=gini, max_depth=15, min_samples_leaf=1, min_samples_split=20 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  criterion=gini, max_depth=15, min_samples_leaf=1, min_samples_split=20, score=0.8575284943011398, total=  14.7s\n",
      "[CV] criterion=gini, max_depth=15, min_samples_leaf=1, min_samples_split=20 \n",
      "[CV]  criterion=gini, max_depth=15, min_samples_leaf=1, min_samples_split=20, score=0.8569928496424821, total=  14.0s\n",
      "[CV] criterion=gini, max_depth=15, min_samples_leaf=1, min_samples_split=20 \n",
      "[CV]  criterion=gini, max_depth=15, min_samples_leaf=1, min_samples_split=20, score=0.8590288543281492, total=  14.2s\n",
      "[CV] criterion=gini, max_depth=15, min_samples_leaf=5, min_samples_split=2 \n",
      "[CV]  criterion=gini, max_depth=15, min_samples_leaf=5, min_samples_split=2, score=0.863377324535093, total=  14.3s\n",
      "[CV] criterion=gini, max_depth=15, min_samples_leaf=5, min_samples_split=2 \n",
      "[CV]  criterion=gini, max_depth=15, min_samples_leaf=5, min_samples_split=2, score=0.8596929846492325, total=  13.6s\n",
      "[CV] criterion=gini, max_depth=15, min_samples_leaf=5, min_samples_split=2 \n",
      "[CV]  criterion=gini, max_depth=15, min_samples_leaf=5, min_samples_split=2, score=0.8665299794969246, total=  13.9s\n",
      "[CV] criterion=gini, max_depth=15, min_samples_leaf=5, min_samples_split=10 \n",
      "[CV]  criterion=gini, max_depth=15, min_samples_leaf=5, min_samples_split=10, score=0.8620275944811038, total=  14.6s\n",
      "[CV] criterion=gini, max_depth=15, min_samples_leaf=5, min_samples_split=10 \n",
      "[CV]  criterion=gini, max_depth=15, min_samples_leaf=5, min_samples_split=10, score=0.8605430271513576, total=  13.8s\n",
      "[CV] criterion=gini, max_depth=15, min_samples_leaf=5, min_samples_split=10 \n",
      "[CV]  criterion=gini, max_depth=15, min_samples_leaf=5, min_samples_split=10, score=0.866079911986798, total=  14.2s\n",
      "[CV] criterion=gini, max_depth=15, min_samples_leaf=5, min_samples_split=20 \n",
      "[CV]  criterion=gini, max_depth=15, min_samples_leaf=5, min_samples_split=20, score=0.8598280343931214, total=  14.4s\n",
      "[CV] criterion=gini, max_depth=15, min_samples_leaf=5, min_samples_split=20 \n",
      "[CV]  criterion=gini, max_depth=15, min_samples_leaf=5, min_samples_split=20, score=0.8576928846442322, total=  14.0s\n",
      "[CV] criterion=gini, max_depth=15, min_samples_leaf=5, min_samples_split=20 \n",
      "[CV]  criterion=gini, max_depth=15, min_samples_leaf=5, min_samples_split=20, score=0.8619792968945342, total=  14.6s\n",
      "[CV] criterion=gini, max_depth=15, min_samples_leaf=10, min_samples_split=2 \n",
      "[CV]  criterion=gini, max_depth=15, min_samples_leaf=10, min_samples_split=2, score=0.8582783443311338, total=  13.3s\n",
      "[CV] criterion=gini, max_depth=15, min_samples_leaf=10, min_samples_split=2 \n",
      "[CV]  criterion=gini, max_depth=15, min_samples_leaf=10, min_samples_split=2, score=0.856292814640732, total=  13.6s\n",
      "[CV] criterion=gini, max_depth=15, min_samples_leaf=10, min_samples_split=2 \n",
      "[CV]  criterion=gini, max_depth=15, min_samples_leaf=10, min_samples_split=2, score=0.8636295444316647, total=  13.1s\n",
      "[CV] criterion=gini, max_depth=15, min_samples_leaf=10, min_samples_split=10 \n",
      "[CV]  criterion=gini, max_depth=15, min_samples_leaf=10, min_samples_split=10, score=0.8580283943211358, total=  13.6s\n",
      "[CV] criterion=gini, max_depth=15, min_samples_leaf=10, min_samples_split=10 \n",
      "[CV]  criterion=gini, max_depth=15, min_samples_leaf=10, min_samples_split=10, score=0.8569428471423571, total=  14.0s\n",
      "[CV] criterion=gini, max_depth=15, min_samples_leaf=10, min_samples_split=10 \n",
      "[CV]  criterion=gini, max_depth=15, min_samples_leaf=10, min_samples_split=10, score=0.8636295444316647, total=  13.6s\n",
      "[CV] criterion=gini, max_depth=15, min_samples_leaf=10, min_samples_split=20 \n",
      "[CV]  criterion=gini, max_depth=15, min_samples_leaf=10, min_samples_split=20, score=0.8582783443311338, total=  13.8s\n",
      "[CV] criterion=gini, max_depth=15, min_samples_leaf=10, min_samples_split=20 \n",
      "[CV]  criterion=gini, max_depth=15, min_samples_leaf=10, min_samples_split=20, score=0.8569428471423571, total=  13.8s\n",
      "[CV] criterion=gini, max_depth=15, min_samples_leaf=10, min_samples_split=20 \n",
      "[CV]  criterion=gini, max_depth=15, min_samples_leaf=10, min_samples_split=20, score=0.8630794619192879, total=  13.5s\n",
      "[CV] criterion=entropy, max_depth=None, min_samples_leaf=1, min_samples_split=2 \n",
      "[CV]  criterion=entropy, max_depth=None, min_samples_leaf=1, min_samples_split=2, score=0.8697260547890422, total=  18.6s\n",
      "[CV] criterion=entropy, max_depth=None, min_samples_leaf=1, min_samples_split=2 \n",
      "[CV]  criterion=entropy, max_depth=None, min_samples_leaf=1, min_samples_split=2, score=0.8658432921646082, total=  19.3s\n",
      "[CV] criterion=entropy, max_depth=None, min_samples_leaf=1, min_samples_split=2 \n",
      "[CV]  criterion=entropy, max_depth=None, min_samples_leaf=1, min_samples_split=2, score=0.8704305645846877, total=  20.2s\n",
      "[CV] criterion=entropy, max_depth=None, min_samples_leaf=1, min_samples_split=10 \n",
      "[CV]  criterion=entropy, max_depth=None, min_samples_leaf=1, min_samples_split=10, score=0.8697260547890422, total=  19.2s\n",
      "[CV] criterion=entropy, max_depth=None, min_samples_leaf=1, min_samples_split=10 \n",
      "[CV]  criterion=entropy, max_depth=None, min_samples_leaf=1, min_samples_split=10, score=0.8633431671583579, total=  14.5s\n",
      "[CV] criterion=entropy, max_depth=None, min_samples_leaf=1, min_samples_split=10 \n",
      "[CV]  criterion=entropy, max_depth=None, min_samples_leaf=1, min_samples_split=10, score=0.8668800320048007, total=  18.4s\n",
      "[CV] criterion=entropy, max_depth=None, min_samples_leaf=1, min_samples_split=20 \n",
      "[CV]  criterion=entropy, max_depth=None, min_samples_leaf=1, min_samples_split=20, score=0.8660267946410718, total=  18.7s\n",
      "[CV] criterion=entropy, max_depth=None, min_samples_leaf=1, min_samples_split=20 \n",
      "[CV]  criterion=entropy, max_depth=None, min_samples_leaf=1, min_samples_split=20, score=0.8587429371468573, total=  18.7s\n",
      "[CV] criterion=entropy, max_depth=None, min_samples_leaf=1, min_samples_split=20 \n",
      "[CV]  criterion=entropy, max_depth=None, min_samples_leaf=1, min_samples_split=20, score=0.8653297994699205, total=  18.7s\n",
      "[CV] criterion=entropy, max_depth=None, min_samples_leaf=5, min_samples_split=2 \n",
      "[CV]  criterion=entropy, max_depth=None, min_samples_leaf=5, min_samples_split=2, score=0.870875824835033, total=  18.4s\n",
      "[CV] criterion=entropy, max_depth=None, min_samples_leaf=5, min_samples_split=2 \n",
      "[CV]  criterion=entropy, max_depth=None, min_samples_leaf=5, min_samples_split=2, score=0.8671433571678584, total=  18.3s\n",
      "[CV] criterion=entropy, max_depth=None, min_samples_leaf=5, min_samples_split=2 \n",
      "[CV]  criterion=entropy, max_depth=None, min_samples_leaf=5, min_samples_split=2, score=0.8697304595689354, total=  18.8s\n",
      "[CV] criterion=entropy, max_depth=None, min_samples_leaf=5, min_samples_split=10 \n",
      "[CV]  criterion=entropy, max_depth=None, min_samples_leaf=5, min_samples_split=10, score=0.8691261747650469, total=  18.6s\n",
      "[CV] criterion=entropy, max_depth=None, min_samples_leaf=5, min_samples_split=10 \n",
      "[CV]  criterion=entropy, max_depth=None, min_samples_leaf=5, min_samples_split=10, score=0.8669933496674834, total=  18.8s\n",
      "[CV] criterion=entropy, max_depth=None, min_samples_leaf=5, min_samples_split=10 \n",
      "[CV]  criterion=entropy, max_depth=None, min_samples_leaf=5, min_samples_split=10, score=0.8699804970745612, total=  18.9s\n",
      "[CV] criterion=entropy, max_depth=None, min_samples_leaf=5, min_samples_split=20 \n",
      "[CV]  criterion=entropy, max_depth=None, min_samples_leaf=5, min_samples_split=20, score=0.8674765046990602, total=  18.3s\n",
      "[CV] criterion=entropy, max_depth=None, min_samples_leaf=5, min_samples_split=20 \n",
      "[CV]  criterion=entropy, max_depth=None, min_samples_leaf=5, min_samples_split=20, score=0.8619430971548577, total=  18.1s\n",
      "[CV] criterion=entropy, max_depth=None, min_samples_leaf=5, min_samples_split=20 \n",
      "[CV]  criterion=entropy, max_depth=None, min_samples_leaf=5, min_samples_split=20, score=0.8667300095014252, total=  18.3s\n",
      "[CV] criterion=entropy, max_depth=None, min_samples_leaf=10, min_samples_split=2 \n",
      "[CV]  criterion=entropy, max_depth=None, min_samples_leaf=10, min_samples_split=2, score=0.8647770445910817, total=  17.6s\n",
      "[CV] criterion=entropy, max_depth=None, min_samples_leaf=10, min_samples_split=2 \n",
      "[CV]  criterion=entropy, max_depth=None, min_samples_leaf=10, min_samples_split=2, score=0.8611930596529827, total=  17.9s\n",
      "[CV] criterion=entropy, max_depth=None, min_samples_leaf=10, min_samples_split=2 \n",
      "[CV]  criterion=entropy, max_depth=None, min_samples_leaf=10, min_samples_split=2, score=0.8641296194429164, total=  18.4s\n",
      "[CV] criterion=entropy, max_depth=None, min_samples_leaf=10, min_samples_split=10 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  criterion=entropy, max_depth=None, min_samples_leaf=10, min_samples_split=10, score=0.8647770445910817, total=  17.8s\n",
      "[CV] criterion=entropy, max_depth=None, min_samples_leaf=10, min_samples_split=10 \n",
      "[CV]  criterion=entropy, max_depth=None, min_samples_leaf=10, min_samples_split=10, score=0.8603430171508576, total=  17.9s\n",
      "[CV] criterion=entropy, max_depth=None, min_samples_leaf=10, min_samples_split=10 \n",
      "[CV]  criterion=entropy, max_depth=None, min_samples_leaf=10, min_samples_split=10, score=0.8646296944541682, total=  18.3s\n",
      "[CV] criterion=entropy, max_depth=None, min_samples_leaf=10, min_samples_split=20 \n",
      "[CV]  criterion=entropy, max_depth=None, min_samples_leaf=10, min_samples_split=20, score=0.8647270545890822, total=  17.6s\n",
      "[CV] criterion=entropy, max_depth=None, min_samples_leaf=10, min_samples_split=20 \n",
      "[CV]  criterion=entropy, max_depth=None, min_samples_leaf=10, min_samples_split=20, score=0.8613430671533576, total=  17.5s\n",
      "[CV] criterion=entropy, max_depth=None, min_samples_leaf=10, min_samples_split=20 \n",
      "[CV]  criterion=entropy, max_depth=None, min_samples_leaf=10, min_samples_split=20, score=0.8637795669350402, total=  18.2s\n",
      "[CV] criterion=entropy, max_depth=5, min_samples_leaf=1, min_samples_split=2 \n",
      "[CV]  criterion=entropy, max_depth=5, min_samples_leaf=1, min_samples_split=2, score=0.6941611677664468, total=   7.5s\n",
      "[CV] criterion=entropy, max_depth=5, min_samples_leaf=1, min_samples_split=2 \n",
      "[CV]  criterion=entropy, max_depth=5, min_samples_leaf=1, min_samples_split=2, score=0.6942347117355868, total=   7.6s\n",
      "[CV] criterion=entropy, max_depth=5, min_samples_leaf=1, min_samples_split=2 \n",
      "[CV]  criterion=entropy, max_depth=5, min_samples_leaf=1, min_samples_split=2, score=0.6902035305295794, total=   7.5s\n",
      "[CV] criterion=entropy, max_depth=5, min_samples_leaf=1, min_samples_split=10 \n",
      "[CV]  criterion=entropy, max_depth=5, min_samples_leaf=1, min_samples_split=10, score=0.6941611677664468, total=   7.5s\n",
      "[CV] criterion=entropy, max_depth=5, min_samples_leaf=1, min_samples_split=10 \n",
      "[CV]  criterion=entropy, max_depth=5, min_samples_leaf=1, min_samples_split=10, score=0.6942347117355868, total=   7.4s\n",
      "[CV] criterion=entropy, max_depth=5, min_samples_leaf=1, min_samples_split=10 \n",
      "[CV]  criterion=entropy, max_depth=5, min_samples_leaf=1, min_samples_split=10, score=0.6902035305295794, total=   7.4s\n",
      "[CV] criterion=entropy, max_depth=5, min_samples_leaf=1, min_samples_split=20 \n",
      "[CV]  criterion=entropy, max_depth=5, min_samples_leaf=1, min_samples_split=20, score=0.6941611677664468, total=   7.8s\n",
      "[CV] criterion=entropy, max_depth=5, min_samples_leaf=1, min_samples_split=20 \n",
      "[CV]  criterion=entropy, max_depth=5, min_samples_leaf=1, min_samples_split=20, score=0.6942347117355868, total=   7.6s\n",
      "[CV] criterion=entropy, max_depth=5, min_samples_leaf=1, min_samples_split=20 \n",
      "[CV]  criterion=entropy, max_depth=5, min_samples_leaf=1, min_samples_split=20, score=0.6902035305295794, total=   7.7s\n",
      "[CV] criterion=entropy, max_depth=5, min_samples_leaf=5, min_samples_split=2 \n",
      "[CV]  criterion=entropy, max_depth=5, min_samples_leaf=5, min_samples_split=2, score=0.6941611677664468, total=   7.4s\n",
      "[CV] criterion=entropy, max_depth=5, min_samples_leaf=5, min_samples_split=2 \n",
      "[CV]  criterion=entropy, max_depth=5, min_samples_leaf=5, min_samples_split=2, score=0.6942347117355868, total=   7.5s\n",
      "[CV] criterion=entropy, max_depth=5, min_samples_leaf=5, min_samples_split=2 \n",
      "[CV]  criterion=entropy, max_depth=5, min_samples_leaf=5, min_samples_split=2, score=0.6902035305295794, total=   7.9s\n",
      "[CV] criterion=entropy, max_depth=5, min_samples_leaf=5, min_samples_split=10 \n",
      "[CV]  criterion=entropy, max_depth=5, min_samples_leaf=5, min_samples_split=10, score=0.6941611677664468, total=   7.6s\n",
      "[CV] criterion=entropy, max_depth=5, min_samples_leaf=5, min_samples_split=10 \n",
      "[CV]  criterion=entropy, max_depth=5, min_samples_leaf=5, min_samples_split=10, score=0.6942347117355868, total=   7.2s\n",
      "[CV] criterion=entropy, max_depth=5, min_samples_leaf=5, min_samples_split=10 \n",
      "[CV]  criterion=entropy, max_depth=5, min_samples_leaf=5, min_samples_split=10, score=0.6902035305295794, total=   7.5s\n",
      "[CV] criterion=entropy, max_depth=5, min_samples_leaf=5, min_samples_split=20 \n",
      "[CV]  criterion=entropy, max_depth=5, min_samples_leaf=5, min_samples_split=20, score=0.6941611677664468, total=   7.7s\n",
      "[CV] criterion=entropy, max_depth=5, min_samples_leaf=5, min_samples_split=20 \n",
      "[CV]  criterion=entropy, max_depth=5, min_samples_leaf=5, min_samples_split=20, score=0.6942347117355868, total=   7.2s\n",
      "[CV] criterion=entropy, max_depth=5, min_samples_leaf=5, min_samples_split=20 \n",
      "[CV]  criterion=entropy, max_depth=5, min_samples_leaf=5, min_samples_split=20, score=0.6902035305295794, total=   7.8s\n",
      "[CV] criterion=entropy, max_depth=5, min_samples_leaf=10, min_samples_split=2 \n",
      "[CV]  criterion=entropy, max_depth=5, min_samples_leaf=10, min_samples_split=2, score=0.6941611677664468, total=   7.5s\n",
      "[CV] criterion=entropy, max_depth=5, min_samples_leaf=10, min_samples_split=2 \n",
      "[CV]  criterion=entropy, max_depth=5, min_samples_leaf=10, min_samples_split=2, score=0.6942347117355868, total=   7.5s\n",
      "[CV] criterion=entropy, max_depth=5, min_samples_leaf=10, min_samples_split=2 \n",
      "[CV]  criterion=entropy, max_depth=5, min_samples_leaf=10, min_samples_split=2, score=0.6902035305295794, total=   7.3s\n",
      "[CV] criterion=entropy, max_depth=5, min_samples_leaf=10, min_samples_split=10 \n",
      "[CV]  criterion=entropy, max_depth=5, min_samples_leaf=10, min_samples_split=10, score=0.6941611677664468, total=   7.7s\n",
      "[CV] criterion=entropy, max_depth=5, min_samples_leaf=10, min_samples_split=10 \n",
      "[CV]  criterion=entropy, max_depth=5, min_samples_leaf=10, min_samples_split=10, score=0.6942347117355868, total=   7.7s\n",
      "[CV] criterion=entropy, max_depth=5, min_samples_leaf=10, min_samples_split=10 \n",
      "[CV]  criterion=entropy, max_depth=5, min_samples_leaf=10, min_samples_split=10, score=0.6902035305295794, total=   7.4s\n",
      "[CV] criterion=entropy, max_depth=5, min_samples_leaf=10, min_samples_split=20 \n",
      "[CV]  criterion=entropy, max_depth=5, min_samples_leaf=10, min_samples_split=20, score=0.6941611677664468, total=   7.1s\n",
      "[CV] criterion=entropy, max_depth=5, min_samples_leaf=10, min_samples_split=20 \n",
      "[CV]  criterion=entropy, max_depth=5, min_samples_leaf=10, min_samples_split=20, score=0.6942347117355868, total=   7.6s\n",
      "[CV] criterion=entropy, max_depth=5, min_samples_leaf=10, min_samples_split=20 \n",
      "[CV]  criterion=entropy, max_depth=5, min_samples_leaf=10, min_samples_split=20, score=0.6902035305295794, total=   7.7s\n",
      "[CV] criterion=entropy, max_depth=10, min_samples_leaf=1, min_samples_split=2 \n",
      "[CV]  criterion=entropy, max_depth=10, min_samples_leaf=1, min_samples_split=2, score=0.8628274345130974, total=  15.9s\n",
      "[CV] criterion=entropy, max_depth=10, min_samples_leaf=1, min_samples_split=2 \n",
      "[CV]  criterion=entropy, max_depth=10, min_samples_leaf=1, min_samples_split=2, score=0.8580429021451073, total=  16.1s\n",
      "[CV] criterion=entropy, max_depth=10, min_samples_leaf=1, min_samples_split=2 \n",
      "[CV]  criterion=entropy, max_depth=10, min_samples_leaf=1, min_samples_split=2, score=0.8599789968495274, total=  16.1s\n",
      "[CV] criterion=entropy, max_depth=10, min_samples_leaf=1, min_samples_split=10 \n",
      "[CV]  criterion=entropy, max_depth=10, min_samples_leaf=1, min_samples_split=10, score=0.8612277544491101, total=  15.9s\n",
      "[CV] criterion=entropy, max_depth=10, min_samples_leaf=1, min_samples_split=10 \n",
      "[CV]  criterion=entropy, max_depth=10, min_samples_leaf=1, min_samples_split=10, score=0.8575928796439822, total=  16.2s\n",
      "[CV] criterion=entropy, max_depth=10, min_samples_leaf=1, min_samples_split=10 \n",
      "[CV]  criterion=entropy, max_depth=10, min_samples_leaf=1, min_samples_split=10, score=0.8596789518427764, total=  16.1s\n",
      "[CV] criterion=entropy, max_depth=10, min_samples_leaf=1, min_samples_split=20 \n",
      "[CV]  criterion=entropy, max_depth=10, min_samples_leaf=1, min_samples_split=20, score=0.8587282543491301, total=  16.6s\n",
      "[CV] criterion=entropy, max_depth=10, min_samples_leaf=1, min_samples_split=20 \n",
      "[CV]  criterion=entropy, max_depth=10, min_samples_leaf=1, min_samples_split=20, score=0.8544427221361068, total=  15.6s\n",
      "[CV] criterion=entropy, max_depth=10, min_samples_leaf=1, min_samples_split=20 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  criterion=entropy, max_depth=10, min_samples_leaf=1, min_samples_split=20, score=0.8563784567685153, total=  16.3s\n",
      "[CV] criterion=entropy, max_depth=10, min_samples_leaf=5, min_samples_split=2 \n",
      "[CV]  criterion=entropy, max_depth=10, min_samples_leaf=5, min_samples_split=2, score=0.862127574485103, total=  15.6s\n",
      "[CV] criterion=entropy, max_depth=10, min_samples_leaf=5, min_samples_split=2 \n",
      "[CV]  criterion=entropy, max_depth=10, min_samples_leaf=5, min_samples_split=2, score=0.8565928296414821, total=  15.5s\n",
      "[CV] criterion=entropy, max_depth=10, min_samples_leaf=5, min_samples_split=2 \n",
      "[CV]  criterion=entropy, max_depth=10, min_samples_leaf=5, min_samples_split=2, score=0.8592288843326499, total=  15.8s\n",
      "[CV] criterion=entropy, max_depth=10, min_samples_leaf=5, min_samples_split=10 \n",
      "[CV]  criterion=entropy, max_depth=10, min_samples_leaf=5, min_samples_split=10, score=0.8622275544891022, total=  15.7s\n",
      "[CV] criterion=entropy, max_depth=10, min_samples_leaf=5, min_samples_split=10 \n",
      "[CV]  criterion=entropy, max_depth=10, min_samples_leaf=5, min_samples_split=10, score=0.8573928696434822, total=  15.7s\n",
      "[CV] criterion=entropy, max_depth=10, min_samples_leaf=5, min_samples_split=10 \n",
      "[CV]  criterion=entropy, max_depth=10, min_samples_leaf=5, min_samples_split=10, score=0.8594289143371505, total=  16.0s\n",
      "[CV] criterion=entropy, max_depth=10, min_samples_leaf=5, min_samples_split=20 \n",
      "[CV]  criterion=entropy, max_depth=10, min_samples_leaf=5, min_samples_split=20, score=0.8594781043791242, total=  15.9s\n",
      "[CV] criterion=entropy, max_depth=10, min_samples_leaf=5, min_samples_split=20 \n",
      "[CV]  criterion=entropy, max_depth=10, min_samples_leaf=5, min_samples_split=20, score=0.8545427271363568, total=  15.7s\n",
      "[CV] criterion=entropy, max_depth=10, min_samples_leaf=5, min_samples_split=20 \n",
      "[CV]  criterion=entropy, max_depth=10, min_samples_leaf=5, min_samples_split=20, score=0.8571285692853928, total=  15.8s\n",
      "[CV] criterion=entropy, max_depth=10, min_samples_leaf=10, min_samples_split=2 \n",
      "[CV]  criterion=entropy, max_depth=10, min_samples_leaf=10, min_samples_split=2, score=0.8581783643271346, total=  15.2s\n",
      "[CV] criterion=entropy, max_depth=10, min_samples_leaf=10, min_samples_split=2 \n",
      "[CV]  criterion=entropy, max_depth=10, min_samples_leaf=10, min_samples_split=2, score=0.8545427271363568, total=  15.3s\n",
      "[CV] criterion=entropy, max_depth=10, min_samples_leaf=10, min_samples_split=2 \n",
      "[CV]  criterion=entropy, max_depth=10, min_samples_leaf=10, min_samples_split=2, score=0.8566284942741411, total=  15.5s\n",
      "[CV] criterion=entropy, max_depth=10, min_samples_leaf=10, min_samples_split=10 \n",
      "[CV]  criterion=entropy, max_depth=10, min_samples_leaf=10, min_samples_split=10, score=0.858378324335133, total=  15.5s\n",
      "[CV] criterion=entropy, max_depth=10, min_samples_leaf=10, min_samples_split=10 \n",
      "[CV]  criterion=entropy, max_depth=10, min_samples_leaf=10, min_samples_split=10, score=0.8541927096354818, total=  15.5s\n",
      "[CV] criterion=entropy, max_depth=10, min_samples_leaf=10, min_samples_split=10 \n",
      "[CV]  criterion=entropy, max_depth=10, min_samples_leaf=10, min_samples_split=10, score=0.8561784267640146, total=  15.4s\n",
      "[CV] criterion=entropy, max_depth=10, min_samples_leaf=10, min_samples_split=20 \n",
      "[CV]  criterion=entropy, max_depth=10, min_samples_leaf=10, min_samples_split=20, score=0.8582783443311338, total=  15.2s\n",
      "[CV] criterion=entropy, max_depth=10, min_samples_leaf=10, min_samples_split=20 \n",
      "[CV]  criterion=entropy, max_depth=10, min_samples_leaf=10, min_samples_split=20, score=0.8542427121356068, total=  15.4s\n",
      "[CV] criterion=entropy, max_depth=10, min_samples_leaf=10, min_samples_split=20 \n",
      "[CV]  criterion=entropy, max_depth=10, min_samples_leaf=10, min_samples_split=20, score=0.8568785317797669, total=  15.6s\n",
      "[CV] criterion=entropy, max_depth=15, min_samples_leaf=1, min_samples_split=2 \n",
      "[CV]  criterion=entropy, max_depth=15, min_samples_leaf=1, min_samples_split=2, score=0.8694261147770446, total=  19.3s\n",
      "[CV] criterion=entropy, max_depth=15, min_samples_leaf=1, min_samples_split=2 \n",
      "[CV]  criterion=entropy, max_depth=15, min_samples_leaf=1, min_samples_split=2, score=0.8657432871643582, total=  19.0s\n",
      "[CV] criterion=entropy, max_depth=15, min_samples_leaf=1, min_samples_split=2 \n",
      "[CV]  criterion=entropy, max_depth=15, min_samples_leaf=1, min_samples_split=2, score=0.8715307296094414, total=  19.1s\n",
      "[CV] criterion=entropy, max_depth=15, min_samples_leaf=1, min_samples_split=10 \n",
      "[CV]  criterion=entropy, max_depth=15, min_samples_leaf=1, min_samples_split=10, score=0.869376124775045, total=  20.2s\n",
      "[CV] criterion=entropy, max_depth=15, min_samples_leaf=1, min_samples_split=10 \n",
      "[CV]  criterion=entropy, max_depth=15, min_samples_leaf=1, min_samples_split=10, score=0.8655432771638581, total=  19.0s\n",
      "[CV] criterion=entropy, max_depth=15, min_samples_leaf=1, min_samples_split=10 \n",
      "[CV]  criterion=entropy, max_depth=15, min_samples_leaf=1, min_samples_split=10, score=0.8701805270790619, total=  19.9s\n",
      "[CV] criterion=entropy, max_depth=15, min_samples_leaf=1, min_samples_split=20 \n",
      "[CV]  criterion=entropy, max_depth=15, min_samples_leaf=1, min_samples_split=20, score=0.8656768646270746, total=  19.4s\n",
      "[CV] criterion=entropy, max_depth=15, min_samples_leaf=1, min_samples_split=20 \n",
      "[CV]  criterion=entropy, max_depth=15, min_samples_leaf=1, min_samples_split=20, score=0.8610930546527327, total=  18.7s\n",
      "[CV] criterion=entropy, max_depth=15, min_samples_leaf=1, min_samples_split=20 \n",
      "[CV]  criterion=entropy, max_depth=15, min_samples_leaf=1, min_samples_split=20, score=0.8656298444766715, total=  19.2s\n",
      "[CV] criterion=entropy, max_depth=15, min_samples_leaf=5, min_samples_split=2 \n",
      "[CV]  criterion=entropy, max_depth=15, min_samples_leaf=5, min_samples_split=2, score=0.8705258948210358, total=  18.7s\n",
      "[CV] criterion=entropy, max_depth=15, min_samples_leaf=5, min_samples_split=2 \n",
      "[CV]  criterion=entropy, max_depth=15, min_samples_leaf=5, min_samples_split=2, score=0.8674433721686085, total=  19.2s\n",
      "[CV] criterion=entropy, max_depth=15, min_samples_leaf=5, min_samples_split=2 \n",
      "[CV]  criterion=entropy, max_depth=15, min_samples_leaf=5, min_samples_split=2, score=0.869030354553183, total=  19.4s\n",
      "[CV] criterion=entropy, max_depth=15, min_samples_leaf=5, min_samples_split=10 \n",
      "[CV]  criterion=entropy, max_depth=15, min_samples_leaf=5, min_samples_split=10, score=0.8714757048590281, total=  18.9s\n",
      "[CV] criterion=entropy, max_depth=15, min_samples_leaf=5, min_samples_split=10 \n",
      "[CV]  criterion=entropy, max_depth=15, min_samples_leaf=5, min_samples_split=10, score=0.8680934046702335, total=  18.9s\n",
      "[CV] criterion=entropy, max_depth=15, min_samples_leaf=5, min_samples_split=10 \n",
      "[CV]  criterion=entropy, max_depth=15, min_samples_leaf=5, min_samples_split=10, score=0.8689303395509327, total=  19.0s\n",
      "[CV] criterion=entropy, max_depth=15, min_samples_leaf=5, min_samples_split=20 \n",
      "[CV]  criterion=entropy, max_depth=15, min_samples_leaf=5, min_samples_split=20, score=0.8669766046790642, total=  19.0s\n",
      "[CV] criterion=entropy, max_depth=15, min_samples_leaf=5, min_samples_split=20 \n",
      "[CV]  criterion=entropy, max_depth=15, min_samples_leaf=5, min_samples_split=20, score=0.8621931096554828, total=  18.8s\n",
      "[CV] criterion=entropy, max_depth=15, min_samples_leaf=5, min_samples_split=20 \n",
      "[CV]  criterion=entropy, max_depth=15, min_samples_leaf=5, min_samples_split=20, score=0.8670800620093014, total=  19.0s\n",
      "[CV] criterion=entropy, max_depth=15, min_samples_leaf=10, min_samples_split=2 \n",
      "[CV]  criterion=entropy, max_depth=15, min_samples_leaf=10, min_samples_split=2, score=0.8638272345530894, total=  18.1s\n",
      "[CV] criterion=entropy, max_depth=15, min_samples_leaf=10, min_samples_split=2 \n",
      "[CV]  criterion=entropy, max_depth=15, min_samples_leaf=10, min_samples_split=2, score=0.8611930596529827, total=  18.3s\n",
      "[CV] criterion=entropy, max_depth=15, min_samples_leaf=10, min_samples_split=2 \n",
      "[CV]  criterion=entropy, max_depth=15, min_samples_leaf=10, min_samples_split=2, score=0.8647797169575436, total=  19.2s\n",
      "[CV] criterion=entropy, max_depth=15, min_samples_leaf=10, min_samples_split=10 \n",
      "[CV]  criterion=entropy, max_depth=15, min_samples_leaf=10, min_samples_split=10, score=0.8642271545690862, total=  18.4s\n",
      "[CV] criterion=entropy, max_depth=15, min_samples_leaf=10, min_samples_split=10 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  criterion=entropy, max_depth=15, min_samples_leaf=10, min_samples_split=10, score=0.8620931046552328, total=  18.5s\n",
      "[CV] criterion=entropy, max_depth=15, min_samples_leaf=10, min_samples_split=10 \n",
      "[CV]  criterion=entropy, max_depth=15, min_samples_leaf=10, min_samples_split=10, score=0.8643796569485422, total=  18.7s\n",
      "[CV] criterion=entropy, max_depth=15, min_samples_leaf=10, min_samples_split=20 \n",
      "[CV]  criterion=entropy, max_depth=15, min_samples_leaf=10, min_samples_split=20, score=0.8645270945810838, total=  17.9s\n",
      "[CV] criterion=entropy, max_depth=15, min_samples_leaf=10, min_samples_split=20 \n",
      "[CV]  criterion=entropy, max_depth=15, min_samples_leaf=10, min_samples_split=20, score=0.8612930646532326, total=  18.4s\n",
      "[CV] criterion=entropy, max_depth=15, min_samples_leaf=10, min_samples_split=20 \n",
      "[CV]  criterion=entropy, max_depth=15, min_samples_leaf=10, min_samples_split=20, score=0.8642296344451668, total=  18.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 216 out of 216 | elapsed: 50.4min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score='raise',\n",
       "       estimator=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "            splitter='best'),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid=[{'criterion': ['gini', 'entropy'], 'min_samples_split': [2, 10, 20], 'max_depth': [None, 5, 10, 15], 'min_samples_leaf': [1, 5, 10]}],\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring='accuracy', verbose=3)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Definindo parâmetros para Decision Trees\n",
    "param_grid_tree = [\n",
    "    {\n",
    "     'criterion': ['gini', 'entropy'],\n",
    "     'min_samples_split': [2, 10, 20],\n",
    "     'max_depth': [None, 5, 10, 15],\n",
    "     'min_samples_leaf': [1, 5, 10]\n",
    "     #'max_leaf_nodes': [None, 5, 10, 20]\n",
    "    }\n",
    "]\n",
    "\n",
    "# Criando classificador\n",
    "dtree = DecisionTreeClassifier()\n",
    "\n",
    "# Treinando e procurando a melhor combinação\n",
    "grid_search_tree = GridSearchCV(dtree, param_grid_tree, cv=3, \n",
    "                           scoring='accuracy', verbose=3)\n",
    "\n",
    "grid_search_tree.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-06T21:50:29.488507Z",
     "start_time": "2019-01-06T21:50:29.481527Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'criterion': 'entropy',\n",
       " 'max_depth': 15,\n",
       " 'min_samples_leaf': 5,\n",
       " 'min_samples_split': 10}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verificando os melhores parâmetros\n",
    "grid_search_tree.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-06T21:50:46.100368Z",
     "start_time": "2019-01-06T21:50:46.094295Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8695"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verificando o melhor score\n",
    "grid_search_tree.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-06T21:50:55.122800Z",
     "start_time": "2019-01-06T21:50:55.113824Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acc_train</th>\n",
       "      <th>acc_train_cv</th>\n",
       "      <th>acc_train_scaled</th>\n",
       "      <th>acc_test</th>\n",
       "      <th>acc_test_scaled</th>\n",
       "      <th>acc_test_grid</th>\n",
       "      <th>acc_test_shifted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [acc_train, acc_train_cv, acc_train_scaled, acc_test, acc_test_scaled, acc_test_grid, acc_test_shifted]\n",
       "Index: []"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-06T21:54:30.407087Z",
     "start_time": "2019-01-06T21:54:30.401102Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'acc_train': 1.0, 'acc_train_cv': 0.8617, 'acc_train_scaled': 0.8604}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_accs_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-06T21:53:29.164909Z",
     "start_time": "2019-01-06T21:53:29.076954Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8878"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = grid_search_tree.best_estimator_\n",
    "p = m.predict(X_test)\n",
    "accuracy_scorecore(y_test, p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dados de Teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-06T22:39:27.916868Z",
     "start_time": "2019-01-06T22:38:53.676825Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia nos dados de teste: 0.8800\n"
     ]
    }
   ],
   "source": [
    "# Calculando acurácia nos dados de teste\n",
    "dtree.fit(X_train, y_train)\n",
    "pred_test = dtree.predict(X_test)\n",
    "\n",
    "acc_test = accuracy_score(y_test, pred_test)\n",
    "print(f'Acurácia nos dados de teste: {acc_test:.4f}')\n",
    "\n",
    "# Salvando valores\n",
    "dict_accs_tree['acc_test'] = round(acc_test, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Realizando uma comparação com a primeira acurácia obtida nos dados de treino, percebe-se que saímos de incríveis (e enganosos) 100% de acurácia para 87,67%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-06T22:01:47.303894Z",
     "start_time": "2019-01-06T22:01:10.234513Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thiagoPanini\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:475: DataConversionWarning: Data with input dtype uint8 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia nos dados de teste após padronização: 0.6226\n"
     ]
    }
   ],
   "source": [
    "# Tentativas de melhora: padronização nos dados\n",
    "X_train_scaled, X_test_scaled = data_scaled(X_train, X_test)\n",
    "\n",
    "dtree.fit(X_train_scaled, y_train)\n",
    "pred_test_scaled = dtree.predict(X_test_scaled)\n",
    "\n",
    "acc_test_scaled = accuracy_score(y_test, pred_test_scaled)\n",
    "print(f'Acurácia nos dados de teste após padronização: {acc_test_scaled:.4f}')\n",
    "\n",
    "# Salvando resultado\n",
    "dict_accs_tree['acc_test_scaled'] = round(acc_test_scaled, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um resultado bem aquém do esperado, provando mais uma vez que o algoritmo ```Decision Trees``` não é sensível à padronização."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-06T22:24:53.738695Z",
     "start_time": "2019-01-06T22:24:26.006363Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia nos dados de teste com os melhores parâmetros: 0.8890\n"
     ]
    }
   ],
   "source": [
    "# Verificação nos dados de teste com os melhores parâmetros (GridSearchCV)\n",
    "dtree_final = grid_search_tree.best_estimator_\n",
    "\n",
    "# Treinando modelo e verificando\n",
    "dtree_final.fit(X_train, y_train)\n",
    "pred_test = dtree_final.predict(X_test)\n",
    "acc_test = accuracy_score(y_test, pred_test)\n",
    "\n",
    "# Comunicando resultado\n",
    "print(f'Acurácia nos dados de teste com os melhores parâmetros: {acc_test:.4f}')\n",
    "\n",
    "# Salvando resultados\n",
    "dict_accs_tree['acc_test_grid'] = round(acc_test, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Houve um ganho de mais de 1% de acurácia através da aplicação do ```GridSearchCV```, o que pode ser considerado excelente! Porém ainda há espaço para mais aprimoramento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shifting Images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este procedimento é um tanto quanto comum em problemas de classificação utilizando imagens. Com ele, é possível aumentar o conjunto de dados de treinamento através do deslocamento de pixels em alguma direção."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-06T22:10:43.805756Z",
     "start_time": "2019-01-06T22:09:58.463040Z"
    }
   },
   "outputs": [],
   "source": [
    "# Retornando novo dataset com imagens deslocadas\n",
    "X_train_augmented, y_train_augmented = augment_data(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-06T22:12:35.592832Z",
     "start_time": "2019-01-06T22:12:35.584853Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensões de X_train: (60000, 784)\n",
      "Dimensões de y_train: (60000,)\n",
      "\n",
      "Dimensões de X_train_augmented: (300000, 784)\n",
      "Dimensões de y_train_augmented: (300000,)\n"
     ]
    }
   ],
   "source": [
    "# Verificando novas dimensões obtidas\n",
    "print(f'Dimensões de X_train: {X_train.shape}')\n",
    "print(f'Dimensões de y_train: {y_train.shape}')\n",
    "print()\n",
    "print(f'Dimensões de X_train_augmented: {X_train_augmented.shape}')\n",
    "print(f'Dimensões de y_train_augmented: {y_train_augmented.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Percebe-se que houve um aumento na quantidade de linhas dos conjuntos de treino."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-06T22:13:46.048064Z",
     "start_time": "2019-01-06T22:13:45.554602Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2080065ac18>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotando gráficos\n",
    "plot_data_augmented(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-06T22:35:56.204629Z",
     "start_time": "2019-01-06T22:33:05.372260Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia após transformação dos dados: 0.9211\n"
     ]
    }
   ],
   "source": [
    "# Realizando o treinamento com novos dados adquiridos\n",
    "dtree_final = grid_search_tree.best_estimator_\n",
    "\n",
    "dtree_final.fit(X_train_augmented, y_train_augmented)\n",
    "\n",
    "# Predizendo valores\n",
    "y_pred = dtree_final.predict(X_test)\n",
    "y_acc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Comunicando resultados\n",
    "print(f'Acurácia após transformação dos dados: {y_acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-06T22:38:30.444154Z",
     "start_time": "2019-01-06T22:38:30.440164Z"
    }
   },
   "outputs": [],
   "source": [
    "# Salvando resultados\n",
    "dict_accs_tree['acc_test_shifted'] = round(y_acc, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-06T22:39:27.925807Z",
     "start_time": "2019-01-06T22:39:27.918824Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'acc_test': 0.88,\n",
       " 'acc_test_grid': 0.889,\n",
       " 'acc_test_scaled': 0.6226,\n",
       " 'acc_test_shifted': 0.9211,\n",
       " 'acc_train': 1.0,\n",
       " 'acc_train_cv': 0.8617,\n",
       " 'acc_train_scaled': 0.8604}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verificando dados salvos até o momento\n",
    "dict_accs_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-06T22:36:40.017894Z",
     "start_time": "2019-01-06T22:36:39.999901Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acc_train</th>\n",
       "      <th>acc_train_cv</th>\n",
       "      <th>acc_train_scaled</th>\n",
       "      <th>acc_test</th>\n",
       "      <th>acc_test_scaled</th>\n",
       "      <th>acc_test_grid</th>\n",
       "      <th>acc_test_shifted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [acc_train, acc_train_cv, acc_train_scaled, acc_test, acc_test_scaled, acc_test_grid, acc_test_shifted]\n",
       "Index: []"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-06T22:39:59.651912Z",
     "start_time": "2019-01-06T22:39:59.071009Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acc_train</th>\n",
       "      <th>acc_train_cv</th>\n",
       "      <th>acc_train_scaled</th>\n",
       "      <th>acc_test</th>\n",
       "      <th>acc_test_scaled</th>\n",
       "      <th>acc_test_grid</th>\n",
       "      <th>acc_test_shifted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.8617</td>\n",
       "      <td>0.8604</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.6226</td>\n",
       "      <td>0.889</td>\n",
       "      <td>0.9211</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   acc_train  acc_train_cv  acc_train_scaled  acc_test  acc_test_scaled  \\\n",
       "0        1.0        0.8617            0.8604      0.88           0.6226   \n",
       "\n",
       "   acc_test_grid  acc_test_shifted  \n",
       "0          0.889            0.9211  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Transformando dados em um objeto DataFrame\n",
    "dataset_accs = dataset_accs.append(dict_accs_tree, ignore_index=True)\n",
    "dataset_accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-06T22:41:18.899555Z",
     "start_time": "2019-01-06T22:41:18.887588Z"
    }
   },
   "outputs": [],
   "source": [
    "# Salvando dados\n",
    "save_dataset(dataset_accs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Em resumo, alguns pontos podem ser levantados sobre o algoritimo ```Decision Tree``` na classificação de digitos manuscritos contidos no dataset MNIST:\n",
    "    - As Árvores de Decisão são muito suscetíveis a Overfitting;\n",
    "    - Padronização nos dados não melhoram a performance do algoritmo;\n",
    "    - A acurácia final (após o deslocamento das imagens) é relativamente baixa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGD Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O segundo algoritmo utilizado será o ```SGDClassifier```. [Doc](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-06T22:42:00.304955Z",
     "start_time": "2019-01-06T22:42:00.289041Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acc_train</th>\n",
       "      <th>acc_train_cv</th>\n",
       "      <th>acc_train_scaled</th>\n",
       "      <th>acc_test</th>\n",
       "      <th>acc_test_scaled</th>\n",
       "      <th>acc_test_grid</th>\n",
       "      <th>acc_test_shifted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.8617</td>\n",
       "      <td>0.8604</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.6226</td>\n",
       "      <td>0.889</td>\n",
       "      <td>0.9211</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   acc_train  acc_train_cv  acc_train_scaled  acc_test  acc_test_scaled  \\\n",
       "0        1.0        0.8617            0.8604      0.88           0.6226   \n",
       "\n",
       "   acc_test_grid  acc_test_shifted  \n",
       "0          0.889            0.9211  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lendo dataset\n",
    "dataset_accs = load_dataset()\n",
    "dataset_accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-06T22:42:20.583492Z",
     "start_time": "2019-01-06T22:42:20.578506Z"
    }
   },
   "outputs": [],
   "source": [
    "# Dicionário criado para armazenar acurácias\n",
    "dict_accs_sgd = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dados de Treino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-06T22:44:36.757821Z",
     "start_time": "2019-01-06T22:44:29.161021Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thiagoPanini\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia obtida com SGD Classifier nos dados de treino: 0.8825\n"
     ]
    }
   ],
   "source": [
    "# Criando classificadoor\n",
    "sgd_clf = SGDClassifier()\n",
    "\n",
    "# Treinando modelo\n",
    "sgd_clf.fit(X_train, y_train)\n",
    "\n",
    "# Predizendo\n",
    "train_pred = sgd_clf.predict(X_train)\n",
    "\n",
    "# Acurácia\n",
    "acc_train = accuracy_score(y_train, train_pred)\n",
    "\n",
    "# Comunicando resultados\n",
    "print(f'Acurácia obtida com SGD Classifier nos dados de treino: {acc_train:.4f}')\n",
    "\n",
    "# Salvando resultados\n",
    "dict_accs_sgd['acc_train'] = round(acc_train, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aplicando ```cross validation``` para verificar a presença de overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-06T22:46:24.791483Z",
     "start_time": "2019-01-06T22:46:11.787421Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thiagoPanini\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\thiagoPanini\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\thiagoPanini\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores: [0.87342531 0.87849392 0.85142771]\n",
      "Média: 0.8678\n",
      "Desvio Padrão: 0.0117\n"
     ]
    }
   ],
   "source": [
    "# Aplicando validação cruzada\n",
    "sgd_scores = cross_val_score(sgd_clf, X_train, y_train,\n",
    "                            cv=3, scoring='accuracy')\n",
    "\n",
    "display_scores(sgd_scores)\n",
    "\n",
    "# Salvando dados\n",
    "dict_accs_sgd['acc_train_cv'] = round(sgd_scores.mean(), 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A queda na performance não foi significativa, indicando assim que o algoritmo de Classificação Stocástica não é tão suscetível ao overfitting. Ainda há espaços para melhorias através da padronização."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-06T22:51:00.654867Z",
     "start_time": "2019-01-06T22:50:38.903640Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thiagoPanini\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:475: DataConversionWarning: Data with input dtype uint8 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\thiagoPanini\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\thiagoPanini\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\thiagoPanini\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "C:\\Users\\thiagoPanini\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores: [0.90696861 0.91109555 0.91233685]\n",
      "Média: 0.9101\n",
      "Desvio Padrão: 0.0023\n"
     ]
    }
   ],
   "source": [
    "# Aplicando a padronização dos dados\n",
    "X_train_scaled, X_test_scaled = data_scaled(X_train, X_test)\n",
    "\n",
    "# Realizando novo treinamento e avaliação\n",
    "sgd_clf.fit(X_train_scaled, y_train)\n",
    "scaled_scores = cross_val_score(sgd_clf, X_train_scaled, y_train,\n",
    "                               cv=3, scoring='accuracy')\n",
    "display_scores(scaled_scores)\n",
    "\n",
    "# Salvando dados\n",
    "dict_accs_sgd['acc_train_scaled'] = round(scaled_scores.mean(), 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Houve uma significativa melhora na performance do algoritmo, indicando que a padronização dos dados pode ser algo bem interessante para modelos treinados com ```SGD Classifier```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tunando Hiperparâmetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-06T23:12:10.909395Z",
     "start_time": "2019-01-06T22:53:03.937127Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 60 candidates, totalling 180 fits\n",
      "[CV] alpha=0.0001, learning_rate=optimal, loss=hinge, penalty=l1 .....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thiagoPanini\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  alpha=0.0001, learning_rate=optimal, loss=hinge, penalty=l1, score=0.9016696660667867, total=  12.1s\n",
      "[CV] alpha=0.0001, learning_rate=optimal, loss=hinge, penalty=l1 .....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   12.2s remaining:    0.0s\n",
      "C:\\Users\\thiagoPanini\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  alpha=0.0001, learning_rate=optimal, loss=hinge, penalty=l1, score=0.9058452922646132, total=  12.2s\n",
      "[CV] alpha=0.0001, learning_rate=optimal, loss=hinge, penalty=l1 .....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:   24.7s remaining:    0.0s\n",
      "C:\\Users\\thiagoPanini\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  alpha=0.0001, learning_rate=optimal, loss=hinge, penalty=l1, score=0.9046857028554283, total=  14.1s\n",
      "[CV] alpha=0.0001, learning_rate=optimal, loss=hinge, penalty=l2 .....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:   39.1s remaining:    0.0s\n",
      "C:\\Users\\thiagoPanini\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  alpha=0.0001, learning_rate=optimal, loss=hinge, penalty=l2, score=0.907118576284743, total=   4.2s\n",
      "[CV] alpha=0.0001, learning_rate=optimal, loss=hinge, penalty=l2 .....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thiagoPanini\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  alpha=0.0001, learning_rate=optimal, loss=hinge, penalty=l2, score=0.9108455422771139, total=   4.4s\n",
      "[CV] alpha=0.0001, learning_rate=optimal, loss=hinge, penalty=l2 .....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thiagoPanini\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  alpha=0.0001, learning_rate=optimal, loss=hinge, penalty=l2, score=0.9110366554983248, total=   3.7s\n",
      "[CV] alpha=0.0001, learning_rate=optimal, loss=perceptron, penalty=l1 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thiagoPanini\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  alpha=0.0001, learning_rate=optimal, loss=perceptron, penalty=l1, score=0.9005698860227954, total=  10.1s\n",
      "[CV] alpha=0.0001, learning_rate=optimal, loss=perceptron, penalty=l1 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thiagoPanini\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  alpha=0.0001, learning_rate=optimal, loss=perceptron, penalty=l1, score=0.9053452672633632, total=   8.2s\n",
      "[CV] alpha=0.0001, learning_rate=optimal, loss=perceptron, penalty=l1 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thiagoPanini\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  alpha=0.0001, learning_rate=optimal, loss=perceptron, penalty=l1, score=0.9017852677901685, total=   7.6s\n",
      "[CV] alpha=0.0001, learning_rate=optimal, loss=perceptron, penalty=l2 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thiagoPanini\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  alpha=0.0001, learning_rate=optimal, loss=perceptron, penalty=l2, score=0.9063687262547491, total=   2.7s\n",
      "[CV] alpha=0.0001, learning_rate=optimal, loss=perceptron, penalty=l2 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thiagoPanini\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  alpha=0.0001, learning_rate=optimal, loss=perceptron, penalty=l2, score=0.9107955397769888, total=   2.6s\n",
      "[CV] alpha=0.0001, learning_rate=optimal, loss=perceptron, penalty=l2 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thiagoPanini\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  alpha=0.0001, learning_rate=optimal, loss=perceptron, penalty=l2, score=0.9110366554983248, total=   2.5s\n",
      "[CV] alpha=0.0001, learning_rate=optimal, loss=log, penalty=l1 .......\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thiagoPanini\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  alpha=0.0001, learning_rate=optimal, loss=log, penalty=l1, score=0.9015696860627874, total=   8.8s\n",
      "[CV] alpha=0.0001, learning_rate=optimal, loss=log, penalty=l1 .......\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thiagoPanini\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  alpha=0.0001, learning_rate=optimal, loss=log, penalty=l1, score=0.9058952947647383, total=   8.8s\n",
      "[CV] alpha=0.0001, learning_rate=optimal, loss=log, penalty=l1 .......\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thiagoPanini\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  alpha=0.0001, learning_rate=optimal, loss=log, penalty=l1, score=0.9036355453317998, total=   8.8s\n",
      "[CV] alpha=0.0001, learning_rate=optimal, loss=log, penalty=l2 .......\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thiagoPanini\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  alpha=0.0001, learning_rate=optimal, loss=log, penalty=l2, score=0.9057188562287543, total=   4.4s\n",
      "[CV] alpha=0.0001, learning_rate=optimal, loss=log, penalty=l2 .......\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thiagoPanini\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  alpha=0.0001, learning_rate=optimal, loss=log, penalty=l2, score=0.9088454422721136, total=   4.7s\n",
      "[CV] alpha=0.0001, learning_rate=optimal, loss=log, penalty=l2 .......\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thiagoPanini\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  alpha=0.0001, learning_rate=optimal, loss=log, penalty=l2, score=0.9113867080062009, total=   4.2s\n",
      "[CV] alpha=0.0012000000000000001, learning_rate=optimal, loss=hinge, penalty=l1 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thiagoPanini\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  alpha=0.0012000000000000001, learning_rate=optimal, loss=hinge, penalty=l1, score=0.8889722055588882, total=   6.6s\n",
      "[CV] alpha=0.0012000000000000001, learning_rate=optimal, loss=hinge, penalty=l1 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thiagoPanini\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  alpha=0.0012000000000000001, learning_rate=optimal, loss=hinge, penalty=l1, score=0.8912945647282364, total=   6.8s\n",
      "[CV] alpha=0.0012000000000000001, learning_rate=optimal, loss=hinge, penalty=l1 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thiagoPanini\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  alpha=0.0012000000000000001, learning_rate=optimal, loss=hinge, penalty=l1, score=0.8887833174976246, total=   6.6s\n",
      "[CV] alpha=0.0012000000000000001, learning_rate=optimal, loss=hinge, penalty=l2 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thiagoPanini\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  alpha=0.0012000000000000001, learning_rate=optimal, loss=hinge, penalty=l2, score=0.8999700059988003, total=   2.6s\n",
      "[CV] alpha=0.0012000000000000001, learning_rate=optimal, loss=hinge, penalty=l2 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thiagoPanini\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  alpha=0.0012000000000000001, learning_rate=optimal, loss=hinge, penalty=l2, score=0.904295214760738, total=   2.7s\n",
      "[CV] alpha=0.0012000000000000001, learning_rate=optimal, loss=hinge, penalty=l2 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thiagoPanini\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  alpha=0.0012000000000000001, learning_rate=optimal, loss=hinge, penalty=l2, score=0.902535380307046, total=   2.7s\n",
      "[CV] alpha=0.0012000000000000001, learning_rate=optimal, loss=perceptron, penalty=l1 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thiagoPanini\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  alpha=0.0012000000000000001, learning_rate=optimal, loss=perceptron, penalty=l1, score=0.8845730853829235, total=   6.7s\n",
      "[CV] alpha=0.0012000000000000001, learning_rate=optimal, loss=perceptron, penalty=l1 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thiagoPanini\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  alpha=0.0012000000000000001, learning_rate=optimal, loss=perceptron, penalty=l1, score=0.888044402220111, total=   6.7s\n",
      "[CV] alpha=0.0012000000000000001, learning_rate=optimal, loss=perceptron, penalty=l1 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thiagoPanini\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  alpha=0.0012000000000000001, learning_rate=optimal, loss=perceptron, penalty=l1, score=0.8854328149222384, total=   7.1s\n",
      "[CV] alpha=0.0012000000000000001, learning_rate=optimal, loss=perceptron, penalty=l2 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thiagoPanini\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  alpha=0.0012000000000000001, learning_rate=optimal, loss=perceptron, penalty=l2, score=0.9004199160167966, total=   2.7s\n",
      "[CV] alpha=0.0012000000000000001, learning_rate=optimal, loss=perceptron, penalty=l2 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thiagoPanini\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  alpha=0.0012000000000000001, learning_rate=optimal, loss=perceptron, penalty=l2, score=0.903895194759738, total=   2.8s\n",
      "[CV] alpha=0.0012000000000000001, learning_rate=optimal, loss=perceptron, penalty=l2 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thiagoPanini\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  alpha=0.0012000000000000001, learning_rate=optimal, loss=perceptron, penalty=l2, score=0.901335200280042, total=   2.6s\n",
      "[CV] alpha=0.0012000000000000001, learning_rate=optimal, loss=log, penalty=l1 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thiagoPanini\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  alpha=0.0012000000000000001, learning_rate=optimal, loss=log, penalty=l1, score=0.889622075584883, total=  13.7s\n",
      "[CV] alpha=0.0012000000000000001, learning_rate=optimal, loss=log, penalty=l1 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thiagoPanini\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  alpha=0.0012000000000000001, learning_rate=optimal, loss=log, penalty=l1, score=0.8920946047302365, total=  13.6s\n",
      "[CV] alpha=0.0012000000000000001, learning_rate=optimal, loss=log, penalty=l1 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thiagoPanini\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  alpha=0.0012000000000000001, learning_rate=optimal, loss=log, penalty=l1, score=0.889183377506626, total=  13.7s\n",
      "[CV] alpha=0.0012000000000000001, learning_rate=optimal, loss=log, penalty=l2 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thiagoPanini\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  alpha=0.0012000000000000001, learning_rate=optimal, loss=log, penalty=l2, score=0.8991701659668067, total=   4.7s\n",
      "[CV] alpha=0.0012000000000000001, learning_rate=optimal, loss=log, penalty=l2 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thiagoPanini\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  alpha=0.0012000000000000001, learning_rate=optimal, loss=log, penalty=l2, score=0.9049452472623631, total=   5.2s\n",
      "[CV] alpha=0.0012000000000000001, learning_rate=optimal, loss=log, penalty=l2 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thiagoPanini\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  alpha=0.0012000000000000001, learning_rate=optimal, loss=log, penalty=l2, score=0.9014852227834175, total=   4.8s\n",
      "[CV] alpha=0.0023, learning_rate=optimal, loss=hinge, penalty=l1 .....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thiagoPanini\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  alpha=0.0023, learning_rate=optimal, loss=hinge, penalty=l1, score=0.8818236352729454, total=   6.7s\n",
      "[CV] alpha=0.0023, learning_rate=optimal, loss=hinge, penalty=l1 .....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thiagoPanini\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  alpha=0.0023, learning_rate=optimal, loss=hinge, penalty=l1, score=0.8874943747187359, total=   6.7s\n",
      "[CV] alpha=0.0023, learning_rate=optimal, loss=hinge, penalty=l1 .....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thiagoPanini\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  alpha=0.0023, learning_rate=optimal, loss=hinge, penalty=l1, score=0.8844326648997349, total=   6.6s\n",
      "[CV] alpha=0.0023, learning_rate=optimal, loss=hinge, penalty=l2 .....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thiagoPanini\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  alpha=0.0023, learning_rate=optimal, loss=hinge, penalty=l2, score=0.896870625874825, total=   2.7s\n",
      "[CV] alpha=0.0023, learning_rate=optimal, loss=hinge, penalty=l2 .....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thiagoPanini\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  alpha=0.0023, learning_rate=optimal, loss=hinge, penalty=l2, score=0.9026951347567378, total=   2.6s\n",
      "[CV] alpha=0.0023, learning_rate=optimal, loss=hinge, penalty=l2 .....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thiagoPanini\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  alpha=0.0023, learning_rate=optimal, loss=hinge, penalty=l2, score=0.9005350802620393, total=   2.6s\n",
      "[CV] alpha=0.0023, learning_rate=optimal, loss=perceptron, penalty=l1 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thiagoPanini\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  alpha=0.0023, learning_rate=optimal, loss=perceptron, penalty=l1, score=0.8760247950409918, total=   6.9s\n",
      "[CV] alpha=0.0023, learning_rate=optimal, loss=perceptron, penalty=l1 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thiagoPanini\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  alpha=0.0023, learning_rate=optimal, loss=perceptron, penalty=l1, score=0.8872443622181109, total=   6.5s\n",
      "[CV] alpha=0.0023, learning_rate=optimal, loss=perceptron, penalty=l1 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thiagoPanini\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  alpha=0.0023, learning_rate=optimal, loss=perceptron, penalty=l1, score=0.8803320498074712, total=   6.8s\n",
      "[CV] alpha=0.0023, learning_rate=optimal, loss=perceptron, penalty=l2 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thiagoPanini\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  alpha=0.0023, learning_rate=optimal, loss=perceptron, penalty=l2, score=0.898870225954809, total=   2.7s\n",
      "[CV] alpha=0.0023, learning_rate=optimal, loss=perceptron, penalty=l2 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thiagoPanini\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  alpha=0.0023, learning_rate=optimal, loss=perceptron, penalty=l2, score=0.8995949797489875, total=   2.9s\n",
      "[CV] alpha=0.0023, learning_rate=optimal, loss=perceptron, penalty=l2 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thiagoPanini\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  alpha=0.0023, learning_rate=optimal, loss=perceptron, penalty=l2, score=0.8991348702305346, total=   2.7s\n",
      "[CV] alpha=0.0023, learning_rate=optimal, loss=log, penalty=l1 .......\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thiagoPanini\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  alpha=0.0023, learning_rate=optimal, loss=log, penalty=l1, score=0.8826734653069386, total=  13.4s\n",
      "[CV] alpha=0.0023, learning_rate=optimal, loss=log, penalty=l1 .......\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thiagoPanini\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  alpha=0.0023, learning_rate=optimal, loss=log, penalty=l1, score=0.8904445222261113, total=  13.7s\n",
      "[CV] alpha=0.0023, learning_rate=optimal, loss=log, penalty=l1 .......\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thiagoPanini\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  alpha=0.0023, learning_rate=optimal, loss=log, penalty=l1, score=0.8827824173626044, total=  13.4s\n",
      "[CV] alpha=0.0023, learning_rate=optimal, loss=log, penalty=l2 .......\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thiagoPanini\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  alpha=0.0023, learning_rate=optimal, loss=log, penalty=l2, score=0.8985702859428114, total=   4.5s\n",
      "[CV] alpha=0.0023, learning_rate=optimal, loss=log, penalty=l2 .......\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thiagoPanini\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  alpha=0.0023, learning_rate=optimal, loss=log, penalty=l2, score=0.9014450722536127, total=   4.6s\n",
      "[CV] alpha=0.0023, learning_rate=optimal, loss=log, penalty=l2 .......\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thiagoPanini\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  alpha=0.0023, learning_rate=optimal, loss=log, penalty=l2, score=0.8992348852327849, total=   4.6s\n",
      "[CV] alpha=0.0034, learning_rate=optimal, loss=hinge, penalty=l1 .....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thiagoPanini\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  alpha=0.0034, learning_rate=optimal, loss=hinge, penalty=l1, score=0.8750749850029994, total=   6.2s\n",
      "[CV] alpha=0.0034, learning_rate=optimal, loss=hinge, penalty=l1 .....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thiagoPanini\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  alpha=0.0034, learning_rate=optimal, loss=hinge, penalty=l1, score=0.8819440972048602, total=   6.2s\n",
      "[CV] alpha=0.0034, learning_rate=optimal, loss=hinge, penalty=l1 .....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thiagoPanini\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  alpha=0.0034, learning_rate=optimal, loss=hinge, penalty=l1, score=0.879681952292844, total=   6.3s\n",
      "[CV] alpha=0.0034, learning_rate=optimal, loss=hinge, penalty=l2 .....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thiagoPanini\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  alpha=0.0034, learning_rate=optimal, loss=hinge, penalty=l2, score=0.8956708658268346, total=   2.6s\n",
      "[CV] alpha=0.0034, learning_rate=optimal, loss=hinge, penalty=l2 .....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thiagoPanini\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  alpha=0.0034, learning_rate=optimal, loss=hinge, penalty=l2, score=0.8982949147457373, total=   2.7s\n",
      "[CV] alpha=0.0034, learning_rate=optimal, loss=hinge, penalty=l2 .....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thiagoPanini\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  alpha=0.0034, learning_rate=optimal, loss=hinge, penalty=l2, score=0.8974346151922789, total=   2.6s\n",
      "[CV] alpha=0.0034, learning_rate=optimal, loss=perceptron, penalty=l1 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thiagoPanini\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  alpha=0.0034, learning_rate=optimal, loss=perceptron, penalty=l1, score=0.8699260147970406, total=   6.4s\n",
      "[CV] alpha=0.0034, learning_rate=optimal, loss=perceptron, penalty=l1 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thiagoPanini\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  alpha=0.0034, learning_rate=optimal, loss=perceptron, penalty=l1, score=0.8831441572078604, total=   6.2s\n",
      "[CV] alpha=0.0034, learning_rate=optimal, loss=perceptron, penalty=l1 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thiagoPanini\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  alpha=0.0034, learning_rate=optimal, loss=perceptron, penalty=l1, score=0.8738310746611991, total=   7.0s\n",
      "[CV] alpha=0.0034, learning_rate=optimal, loss=perceptron, penalty=l2 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thiagoPanini\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  alpha=0.0034, learning_rate=optimal, loss=perceptron, penalty=l2, score=0.8959208158368326, total=   2.6s\n",
      "[CV] alpha=0.0034, learning_rate=optimal, loss=perceptron, penalty=l2 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thiagoPanini\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  alpha=0.0034, learning_rate=optimal, loss=perceptron, penalty=l2, score=0.9001450072503625, total=   2.6s\n",
      "[CV] alpha=0.0034, learning_rate=optimal, loss=perceptron, penalty=l2 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thiagoPanini\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  alpha=0.0034, learning_rate=optimal, loss=perceptron, penalty=l2, score=0.898384757713657, total=   2.7s\n",
      "[CV] alpha=0.0034, learning_rate=optimal, loss=log, penalty=l1 .......\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thiagoPanini\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  alpha=0.0034, learning_rate=optimal, loss=log, penalty=l1, score=0.8784743051389722, total=  13.7s\n",
      "[CV] alpha=0.0034, learning_rate=optimal, loss=log, penalty=l1 .......\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thiagoPanini\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  alpha=0.0034, learning_rate=optimal, loss=log, penalty=l1, score=0.8821441072053603, total=  12.7s\n",
      "[CV] alpha=0.0034, learning_rate=optimal, loss=log, penalty=l1 .......\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thiagoPanini\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  alpha=0.0034, learning_rate=optimal, loss=log, penalty=l1, score=0.8774316147422113, total=  13.6s\n",
      "[CV] alpha=0.0034, learning_rate=optimal, loss=log, penalty=l2 .......\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thiagoPanini\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  alpha=0.0034, learning_rate=optimal, loss=log, penalty=l2, score=0.8955208958208358, total=   4.5s\n",
      "[CV] alpha=0.0034, learning_rate=optimal, loss=log, penalty=l2 .......\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thiagoPanini\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  alpha=0.0034, learning_rate=optimal, loss=log, penalty=l2, score=0.9023951197559879, total=   4.5s\n",
      "[CV] alpha=0.0034, learning_rate=optimal, loss=log, penalty=l2 .......\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thiagoPanini\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  alpha=0.0034, learning_rate=optimal, loss=log, penalty=l2, score=0.8999349902485373, total=   4.6s\n",
      "[CV] alpha=0.0045000000000000005, learning_rate=optimal, loss=hinge, penalty=l1 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thiagoPanini\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  alpha=0.0045000000000000005, learning_rate=optimal, loss=hinge, penalty=l1, score=0.8671265746850629, total=   6.5s\n",
      "[CV] alpha=0.0045000000000000005, learning_rate=optimal, loss=hinge, penalty=l1 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thiagoPanini\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  alpha=0.0045000000000000005, learning_rate=optimal, loss=hinge, penalty=l1, score=0.8814940747037352, total=   6.2s\n",
      "[CV] alpha=0.0045000000000000005, learning_rate=optimal, loss=hinge, penalty=l1 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thiagoPanini\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  alpha=0.0045000000000000005, learning_rate=optimal, loss=hinge, penalty=l1, score=0.8737310596589488, total=   6.2s\n",
      "[CV] alpha=0.0045000000000000005, learning_rate=optimal, loss=hinge, penalty=l2 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thiagoPanini\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  alpha=0.0045000000000000005, learning_rate=optimal, loss=hinge, penalty=l2, score=0.895870825834833, total=   2.6s\n",
      "[CV] alpha=0.0045000000000000005, learning_rate=optimal, loss=hinge, penalty=l2 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thiagoPanini\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  alpha=0.0045000000000000005, learning_rate=optimal, loss=hinge, penalty=l2, score=0.9006950347517376, total=   2.6s\n",
      "[CV] alpha=0.0045000000000000005, learning_rate=optimal, loss=hinge, penalty=l2 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thiagoPanini\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  alpha=0.0045000000000000005, learning_rate=optimal, loss=hinge, penalty=l2, score=0.8955843376506476, total=   2.7s\n",
      "[CV] alpha=0.0045000000000000005, learning_rate=optimal, loss=perceptron, penalty=l1 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thiagoPanini\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  alpha=0.0045000000000000005, learning_rate=optimal, loss=perceptron, penalty=l1, score=0.8674765046990602, total=   6.1s\n",
      "[CV] alpha=0.0045000000000000005, learning_rate=optimal, loss=perceptron, penalty=l1 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thiagoPanini\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  alpha=0.0045000000000000005, learning_rate=optimal, loss=perceptron, penalty=l1, score=0.8789939496974849, total=   6.2s\n",
      "[CV] alpha=0.0045000000000000005, learning_rate=optimal, loss=perceptron, penalty=l1 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thiagoPanini\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  alpha=0.0045000000000000005, learning_rate=optimal, loss=perceptron, penalty=l1, score=0.8738810821623243, total=   6.2s\n",
      "[CV] alpha=0.0045000000000000005, learning_rate=optimal, loss=perceptron, penalty=l2 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thiagoPanini\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  alpha=0.0045000000000000005, learning_rate=optimal, loss=perceptron, penalty=l2, score=0.8945210957808438, total=   2.7s\n",
      "[CV] alpha=0.0045000000000000005, learning_rate=optimal, loss=perceptron, penalty=l2 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thiagoPanini\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  alpha=0.0045000000000000005, learning_rate=optimal, loss=perceptron, penalty=l2, score=0.8997449872493625, total=   2.6s\n",
      "[CV] alpha=0.0045000000000000005, learning_rate=optimal, loss=perceptron, penalty=l2 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thiagoPanini\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  alpha=0.0045000000000000005, learning_rate=optimal, loss=perceptron, penalty=l2, score=0.8966344951742762, total=   2.6s\n",
      "[CV] alpha=0.0045000000000000005, learning_rate=optimal, loss=log, penalty=l1 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thiagoPanini\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  alpha=0.0045000000000000005, learning_rate=optimal, loss=log, penalty=l1, score=0.8745250949810038, total=  13.3s\n",
      "[CV] alpha=0.0045000000000000005, learning_rate=optimal, loss=log, penalty=l1 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thiagoPanini\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  alpha=0.0045000000000000005, learning_rate=optimal, loss=log, penalty=l1, score=0.8743937196859843, total=  13.0s\n",
      "[CV] alpha=0.0045000000000000005, learning_rate=optimal, loss=log, penalty=l1 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thiagoPanini\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  alpha=0.0045000000000000005, learning_rate=optimal, loss=log, penalty=l1, score=0.8698804820723108, total=  12.7s\n",
      "[CV] alpha=0.0045000000000000005, learning_rate=optimal, loss=log, penalty=l2 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thiagoPanini\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  alpha=0.0045000000000000005, learning_rate=optimal, loss=log, penalty=l2, score=0.8937712457508499, total=   4.5s\n",
      "[CV] alpha=0.0045000000000000005, learning_rate=optimal, loss=log, penalty=l2 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thiagoPanini\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  alpha=0.0045000000000000005, learning_rate=optimal, loss=log, penalty=l2, score=0.8998449922496125, total=   4.6s\n",
      "[CV] alpha=0.0045000000000000005, learning_rate=optimal, loss=log, penalty=l2 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thiagoPanini\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  alpha=0.0045000000000000005, learning_rate=optimal, loss=log, penalty=l2, score=0.8934340151022654, total=   4.5s\n",
      "[CV] alpha=0.005600000000000001, learning_rate=optimal, loss=hinge, penalty=l1 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thiagoPanini\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  alpha=0.005600000000000001, learning_rate=optimal, loss=hinge, penalty=l1, score=0.8586782643471306, total=   6.2s\n",
      "[CV] alpha=0.005600000000000001, learning_rate=optimal, loss=hinge, penalty=l1 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thiagoPanini\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  alpha=0.005600000000000001, learning_rate=optimal, loss=hinge, penalty=l1, score=0.8745437271863593, total=   6.2s\n",
      "[CV] alpha=0.005600000000000001, learning_rate=optimal, loss=hinge, penalty=l1 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thiagoPanini\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  alpha=0.005600000000000001, learning_rate=optimal, loss=hinge, penalty=l1, score=0.8723808571285693, total=   6.2s\n",
      "[CV] alpha=0.005600000000000001, learning_rate=optimal, loss=hinge, penalty=l2 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thiagoPanini\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  alpha=0.005600000000000001, learning_rate=optimal, loss=hinge, penalty=l2, score=0.8948210357928414, total=   2.7s\n",
      "[CV] alpha=0.005600000000000001, learning_rate=optimal, loss=hinge, penalty=l2 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thiagoPanini\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  alpha=0.005600000000000001, learning_rate=optimal, loss=hinge, penalty=l2, score=0.8983449172458623, total=   2.6s\n",
      "[CV] alpha=0.005600000000000001, learning_rate=optimal, loss=hinge, penalty=l2 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thiagoPanini\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  alpha=0.005600000000000001, learning_rate=optimal, loss=hinge, penalty=l2, score=0.895134270140521, total=   2.6s\n",
      "[CV] alpha=0.005600000000000001, learning_rate=optimal, loss=perceptron, penalty=l1 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thiagoPanini\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  alpha=0.005600000000000001, learning_rate=optimal, loss=perceptron, penalty=l1, score=0.8622775444911018, total=   6.4s\n",
      "[CV] alpha=0.005600000000000001, learning_rate=optimal, loss=perceptron, penalty=l1 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thiagoPanini\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  alpha=0.005600000000000001, learning_rate=optimal, loss=perceptron, penalty=l1, score=0.8705435271763589, total=   6.3s\n",
      "[CV] alpha=0.005600000000000001, learning_rate=optimal, loss=perceptron, penalty=l1 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thiagoPanini\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  alpha=0.005600000000000001, learning_rate=optimal, loss=perceptron, penalty=l1, score=0.8642796419462919, total=   6.3s\n",
      "[CV] alpha=0.005600000000000001, learning_rate=optimal, loss=perceptron, penalty=l2 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thiagoPanini\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  alpha=0.005600000000000001, learning_rate=optimal, loss=perceptron, penalty=l2, score=0.8947710457908419, total=   2.6s\n",
      "[CV] alpha=0.005600000000000001, learning_rate=optimal, loss=perceptron, penalty=l2 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thiagoPanini\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  alpha=0.005600000000000001, learning_rate=optimal, loss=perceptron, penalty=l2, score=0.8984949247462373, total=   2.6s\n",
      "[CV] alpha=0.005600000000000001, learning_rate=optimal, loss=perceptron, penalty=l2 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thiagoPanini\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  alpha=0.005600000000000001, learning_rate=optimal, loss=perceptron, penalty=l2, score=0.893934090113517, total=   2.8s\n",
      "[CV] alpha=0.005600000000000001, learning_rate=optimal, loss=log, penalty=l1 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thiagoPanini\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  alpha=0.005600000000000001, learning_rate=optimal, loss=log, penalty=l1, score=0.865626874625075, total=  12.2s\n",
      "[CV] alpha=0.005600000000000001, learning_rate=optimal, loss=log, penalty=l1 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thiagoPanini\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  alpha=0.005600000000000001, learning_rate=optimal, loss=log, penalty=l1, score=0.8747437371868594, total=  13.4s\n",
      "[CV] alpha=0.005600000000000001, learning_rate=optimal, loss=log, penalty=l1 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thiagoPanini\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  alpha=0.005600000000000001, learning_rate=optimal, loss=log, penalty=l1, score=0.8695804370655599, total=  12.4s\n",
      "[CV] alpha=0.005600000000000001, learning_rate=optimal, loss=log, penalty=l2 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thiagoPanini\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  alpha=0.005600000000000001, learning_rate=optimal, loss=log, penalty=l2, score=0.8941711657668466, total=   4.6s\n",
      "[CV] alpha=0.005600000000000001, learning_rate=optimal, loss=log, penalty=l2 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thiagoPanini\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  alpha=0.005600000000000001, learning_rate=optimal, loss=log, penalty=l2, score=0.8996449822491125, total=   4.5s\n",
      "[CV] alpha=0.005600000000000001, learning_rate=optimal, loss=log, penalty=l2 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thiagoPanini\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  alpha=0.005600000000000001, learning_rate=optimal, loss=log, penalty=l2, score=0.8942341351202681, total=   4.6s\n",
      "[CV] alpha=0.0067, learning_rate=optimal, loss=hinge, penalty=l1 .....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thiagoPanini\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  alpha=0.0067, learning_rate=optimal, loss=hinge, penalty=l1, score=0.8586782643471306, total=   5.9s\n",
      "[CV] alpha=0.0067, learning_rate=optimal, loss=hinge, penalty=l1 .....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thiagoPanini\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  alpha=0.0067, learning_rate=optimal, loss=hinge, penalty=l1, score=0.8697434871743587, total=   6.0s\n",
      "[CV] alpha=0.0067, learning_rate=optimal, loss=hinge, penalty=l1 .....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thiagoPanini\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  alpha=0.0067, learning_rate=optimal, loss=hinge, penalty=l1, score=0.8593789068360254, total=   6.0s\n",
      "[CV] alpha=0.0067, learning_rate=optimal, loss=hinge, penalty=l2 .....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thiagoPanini\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  alpha=0.0067, learning_rate=optimal, loss=hinge, penalty=l2, score=0.8930713857228554, total=   2.6s\n",
      "[CV] alpha=0.0067, learning_rate=optimal, loss=hinge, penalty=l2 .....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thiagoPanini\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  alpha=0.0067, learning_rate=optimal, loss=hinge, penalty=l2, score=0.8983449172458623, total=   2.6s\n",
      "[CV] alpha=0.0067, learning_rate=optimal, loss=hinge, penalty=l2 .....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thiagoPanini\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  alpha=0.0067, learning_rate=optimal, loss=hinge, penalty=l2, score=0.8975346301945292, total=   2.6s\n",
      "[CV] alpha=0.0067, learning_rate=optimal, loss=perceptron, penalty=l1 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thiagoPanini\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  alpha=0.0067, learning_rate=optimal, loss=perceptron, penalty=l1, score=0.8614277144571085, total=   6.0s\n",
      "[CV] alpha=0.0067, learning_rate=optimal, loss=perceptron, penalty=l1 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thiagoPanini\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  alpha=0.0067, learning_rate=optimal, loss=perceptron, penalty=l1, score=0.8642932146607331, total=   6.0s\n",
      "[CV] alpha=0.0067, learning_rate=optimal, loss=perceptron, penalty=l1 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thiagoPanini\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  alpha=0.0067, learning_rate=optimal, loss=perceptron, penalty=l1, score=0.8612791918787818, total=   5.9s\n",
      "[CV] alpha=0.0067, learning_rate=optimal, loss=perceptron, penalty=l2 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thiagoPanini\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  alpha=0.0067, learning_rate=optimal, loss=perceptron, penalty=l2, score=0.8935212957408518, total=   2.6s\n",
      "[CV] alpha=0.0067, learning_rate=optimal, loss=perceptron, penalty=l2 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thiagoPanini\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  alpha=0.0067, learning_rate=optimal, loss=perceptron, penalty=l2, score=0.8979948997449873, total=   2.7s\n",
      "[CV] alpha=0.0067, learning_rate=optimal, loss=perceptron, penalty=l2 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thiagoPanini\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  alpha=0.0067, learning_rate=optimal, loss=perceptron, penalty=l2, score=0.8959343901585238, total=   2.6s\n",
      "[CV] alpha=0.0067, learning_rate=optimal, loss=log, penalty=l1 .......\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thiagoPanini\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  alpha=0.0067, learning_rate=optimal, loss=log, penalty=l1, score=0.857878424315137, total=  12.1s\n",
      "[CV] alpha=0.0067, learning_rate=optimal, loss=log, penalty=l1 .......\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thiagoPanini\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  alpha=0.0067, learning_rate=optimal, loss=log, penalty=l1, score=0.8664933246662333, total=  12.0s\n",
      "[CV] alpha=0.0067, learning_rate=optimal, loss=log, penalty=l1 .......\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thiagoPanini\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  alpha=0.0067, learning_rate=optimal, loss=log, penalty=l1, score=0.8627294094114117, total=  13.0s\n",
      "[CV] alpha=0.0067, learning_rate=optimal, loss=log, penalty=l2 .......\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thiagoPanini\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  alpha=0.0067, learning_rate=optimal, loss=log, penalty=l2, score=0.8914717056588682, total=   4.6s\n",
      "[CV] alpha=0.0067, learning_rate=optimal, loss=log, penalty=l2 .......\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thiagoPanini\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  alpha=0.0067, learning_rate=optimal, loss=log, penalty=l2, score=0.8952447622381119, total=   4.5s\n",
      "[CV] alpha=0.0067, learning_rate=optimal, loss=log, penalty=l2 .......\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thiagoPanini\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  alpha=0.0067, learning_rate=optimal, loss=log, penalty=l2, score=0.8946341951292693, total=   4.5s\n",
      "[CV] alpha=0.0078000000000000005, learning_rate=optimal, loss=hinge, penalty=l1 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thiagoPanini\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  alpha=0.0078000000000000005, learning_rate=optimal, loss=hinge, penalty=l1, score=0.858878224355129, total=   5.9s\n",
      "[CV] alpha=0.0078000000000000005, learning_rate=optimal, loss=hinge, penalty=l1 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thiagoPanini\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  alpha=0.0078000000000000005, learning_rate=optimal, loss=hinge, penalty=l1, score=0.8598429921496075, total=   5.8s\n",
      "[CV] alpha=0.0078000000000000005, learning_rate=optimal, loss=hinge, penalty=l1 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thiagoPanini\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  alpha=0.0078000000000000005, learning_rate=optimal, loss=hinge, penalty=l1, score=0.8499274891233685, total=   6.0s\n",
      "[CV] alpha=0.0078000000000000005, learning_rate=optimal, loss=hinge, penalty=l2 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thiagoPanini\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  alpha=0.0078000000000000005, learning_rate=optimal, loss=hinge, penalty=l2, score=0.8930713857228554, total=   2.6s\n",
      "[CV] alpha=0.0078000000000000005, learning_rate=optimal, loss=hinge, penalty=l2 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thiagoPanini\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  alpha=0.0078000000000000005, learning_rate=optimal, loss=hinge, penalty=l2, score=0.8993949697484874, total=   2.6s\n",
      "[CV] alpha=0.0078000000000000005, learning_rate=optimal, loss=hinge, penalty=l2 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thiagoPanini\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  alpha=0.0078000000000000005, learning_rate=optimal, loss=hinge, penalty=l2, score=0.8947842176326449, total=   2.6s\n",
      "[CV] alpha=0.0078000000000000005, learning_rate=optimal, loss=perceptron, penalty=l1 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thiagoPanini\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  alpha=0.0078000000000000005, learning_rate=optimal, loss=perceptron, penalty=l1, score=0.8482803439312138, total=   5.8s\n",
      "[CV] alpha=0.0078000000000000005, learning_rate=optimal, loss=perceptron, penalty=l1 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thiagoPanini\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  alpha=0.0078000000000000005, learning_rate=optimal, loss=perceptron, penalty=l1, score=0.8662933146657333, total=   6.9s\n",
      "[CV] alpha=0.0078000000000000005, learning_rate=optimal, loss=perceptron, penalty=l1 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thiagoPanini\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  alpha=0.0078000000000000005, learning_rate=optimal, loss=perceptron, penalty=l1, score=0.8466269940491074, total=   7.1s\n",
      "[CV] alpha=0.0078000000000000005, learning_rate=optimal, loss=perceptron, penalty=l2 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thiagoPanini\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  alpha=0.0078000000000000005, learning_rate=optimal, loss=perceptron, penalty=l2, score=0.8924715056988602, total=   3.0s\n",
      "[CV] alpha=0.0078000000000000005, learning_rate=optimal, loss=perceptron, penalty=l2 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thiagoPanini\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  alpha=0.0078000000000000005, learning_rate=optimal, loss=perceptron, penalty=l2, score=0.8973948697434871, total=   3.3s\n",
      "[CV] alpha=0.0078000000000000005, learning_rate=optimal, loss=perceptron, penalty=l2 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thiagoPanini\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  alpha=0.0078000000000000005, learning_rate=optimal, loss=perceptron, penalty=l2, score=0.8951842776416462, total=   3.6s\n",
      "[CV] alpha=0.0078000000000000005, learning_rate=optimal, loss=log, penalty=l1 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thiagoPanini\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  alpha=0.0078000000000000005, learning_rate=optimal, loss=log, penalty=l1, score=0.8539292141571686, total=  14.8s\n",
      "[CV] alpha=0.0078000000000000005, learning_rate=optimal, loss=log, penalty=l1 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thiagoPanini\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  alpha=0.0078000000000000005, learning_rate=optimal, loss=log, penalty=l1, score=0.8621931096554828, total=  12.6s\n",
      "[CV] alpha=0.0078000000000000005, learning_rate=optimal, loss=log, penalty=l1 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thiagoPanini\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  alpha=0.0078000000000000005, learning_rate=optimal, loss=log, penalty=l1, score=0.8598789818472771, total=  12.5s\n",
      "[CV] alpha=0.0078000000000000005, learning_rate=optimal, loss=log, penalty=l2 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thiagoPanini\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  alpha=0.0078000000000000005, learning_rate=optimal, loss=log, penalty=l2, score=0.8922215556888622, total=   5.0s\n",
      "[CV] alpha=0.0078000000000000005, learning_rate=optimal, loss=log, penalty=l2 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thiagoPanini\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  alpha=0.0078000000000000005, learning_rate=optimal, loss=log, penalty=l2, score=0.8989949497474874, total=   4.8s\n",
      "[CV] alpha=0.0078000000000000005, learning_rate=optimal, loss=log, penalty=l2 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thiagoPanini\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  alpha=0.0078000000000000005, learning_rate=optimal, loss=log, penalty=l2, score=0.8936840526078912, total=   7.3s\n",
      "[CV] alpha=0.0089, learning_rate=optimal, loss=hinge, penalty=l1 .....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thiagoPanini\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  alpha=0.0089, learning_rate=optimal, loss=hinge, penalty=l1, score=0.8473305338932213, total=   7.1s\n",
      "[CV] alpha=0.0089, learning_rate=optimal, loss=hinge, penalty=l1 .....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thiagoPanini\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  alpha=0.0089, learning_rate=optimal, loss=hinge, penalty=l1, score=0.8544427221361068, total=   6.7s\n",
      "[CV] alpha=0.0089, learning_rate=optimal, loss=hinge, penalty=l1 .....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thiagoPanini\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  alpha=0.0089, learning_rate=optimal, loss=hinge, penalty=l1, score=0.8590288543281492, total=   7.5s\n",
      "[CV] alpha=0.0089, learning_rate=optimal, loss=hinge, penalty=l2 .....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thiagoPanini\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  alpha=0.0089, learning_rate=optimal, loss=hinge, penalty=l2, score=0.8939212157568486, total=   2.8s\n",
      "[CV] alpha=0.0089, learning_rate=optimal, loss=hinge, penalty=l2 .....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thiagoPanini\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  alpha=0.0089, learning_rate=optimal, loss=hinge, penalty=l2, score=0.8975948797439872, total=   3.0s\n",
      "[CV] alpha=0.0089, learning_rate=optimal, loss=hinge, penalty=l2 .....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thiagoPanini\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  alpha=0.0089, learning_rate=optimal, loss=hinge, penalty=l2, score=0.8943341501225184, total=   2.8s\n",
      "[CV] alpha=0.0089, learning_rate=optimal, loss=perceptron, penalty=l1 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thiagoPanini\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  alpha=0.0089, learning_rate=optimal, loss=perceptron, penalty=l1, score=0.8552289542091581, total=   6.4s\n",
      "[CV] alpha=0.0089, learning_rate=optimal, loss=perceptron, penalty=l1 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thiagoPanini\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  alpha=0.0089, learning_rate=optimal, loss=perceptron, penalty=l1, score=0.8454922746137307, total=   6.7s\n",
      "[CV] alpha=0.0089, learning_rate=optimal, loss=perceptron, penalty=l1 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thiagoPanini\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  alpha=0.0089, learning_rate=optimal, loss=perceptron, penalty=l1, score=0.8529279391908786, total=   7.1s\n",
      "[CV] alpha=0.0089, learning_rate=optimal, loss=perceptron, penalty=l2 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thiagoPanini\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  alpha=0.0089, learning_rate=optimal, loss=perceptron, penalty=l2, score=0.8919716056788642, total=   2.8s\n",
      "[CV] alpha=0.0089, learning_rate=optimal, loss=perceptron, penalty=l2 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thiagoPanini\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  alpha=0.0089, learning_rate=optimal, loss=perceptron, penalty=l2, score=0.8970948547427371, total=   3.0s\n",
      "[CV] alpha=0.0089, learning_rate=optimal, loss=perceptron, penalty=l2 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thiagoPanini\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  alpha=0.0089, learning_rate=optimal, loss=perceptron, penalty=l2, score=0.8934340151022654, total=   2.8s\n",
      "[CV] alpha=0.0089, learning_rate=optimal, loss=log, penalty=l1 .......\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thiagoPanini\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  alpha=0.0089, learning_rate=optimal, loss=log, penalty=l1, score=0.8420315936812638, total=  12.3s\n",
      "[CV] alpha=0.0089, learning_rate=optimal, loss=log, penalty=l1 .......\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thiagoPanini\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  alpha=0.0089, learning_rate=optimal, loss=log, penalty=l1, score=0.8611430571528577, total=  12.6s\n",
      "[CV] alpha=0.0089, learning_rate=optimal, loss=log, penalty=l1 .......\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thiagoPanini\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  alpha=0.0089, learning_rate=optimal, loss=log, penalty=l1, score=0.8482272340851128, total=  12.6s\n",
      "[CV] alpha=0.0089, learning_rate=optimal, loss=log, penalty=l2 .......\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thiagoPanini\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  alpha=0.0089, learning_rate=optimal, loss=log, penalty=l2, score=0.890621875624875, total=   4.8s\n",
      "[CV] alpha=0.0089, learning_rate=optimal, loss=log, penalty=l2 .......\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thiagoPanini\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  alpha=0.0089, learning_rate=optimal, loss=log, penalty=l2, score=0.896294814740737, total=   4.9s\n",
      "[CV] alpha=0.0089, learning_rate=optimal, loss=log, penalty=l2 .......\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thiagoPanini\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  alpha=0.0089, learning_rate=optimal, loss=log, penalty=l2, score=0.8937840676101415, total=   4.7s\n",
      "[CV] alpha=0.01, learning_rate=optimal, loss=hinge, penalty=l1 .......\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thiagoPanini\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  alpha=0.01, learning_rate=optimal, loss=hinge, penalty=l1, score=0.8380323935212958, total=   6.3s\n",
      "[CV] alpha=0.01, learning_rate=optimal, loss=hinge, penalty=l1 .......\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thiagoPanini\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  alpha=0.01, learning_rate=optimal, loss=hinge, penalty=l1, score=0.8521926096304815, total=   6.1s\n",
      "[CV] alpha=0.01, learning_rate=optimal, loss=hinge, penalty=l1 .......\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thiagoPanini\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  alpha=0.01, learning_rate=optimal, loss=hinge, penalty=l1, score=0.8454768215232285, total=   6.5s\n",
      "[CV] alpha=0.01, learning_rate=optimal, loss=hinge, penalty=l2 .......\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thiagoPanini\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  alpha=0.01, learning_rate=optimal, loss=hinge, penalty=l2, score=0.8941711657668466, total=   2.9s\n",
      "[CV] alpha=0.01, learning_rate=optimal, loss=hinge, penalty=l2 .......\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thiagoPanini\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  alpha=0.01, learning_rate=optimal, loss=hinge, penalty=l2, score=0.8955447772388619, total=   2.9s\n",
      "[CV] alpha=0.01, learning_rate=optimal, loss=hinge, penalty=l2 .......\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thiagoPanini\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  alpha=0.01, learning_rate=optimal, loss=hinge, penalty=l2, score=0.8958343751562734, total=   3.2s\n",
      "[CV] alpha=0.01, learning_rate=optimal, loss=perceptron, penalty=l1 ..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thiagoPanini\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  alpha=0.01, learning_rate=optimal, loss=perceptron, penalty=l1, score=0.8447310537892422, total=   6.2s\n",
      "[CV] alpha=0.01, learning_rate=optimal, loss=perceptron, penalty=l1 ..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thiagoPanini\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  alpha=0.01, learning_rate=optimal, loss=perceptron, penalty=l1, score=0.8523926196309816, total=   6.5s\n",
      "[CV] alpha=0.01, learning_rate=optimal, loss=perceptron, penalty=l1 ..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thiagoPanini\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  alpha=0.01, learning_rate=optimal, loss=perceptron, penalty=l1, score=0.8435265289793469, total=   6.9s\n",
      "[CV] alpha=0.01, learning_rate=optimal, loss=perceptron, penalty=l2 ..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thiagoPanini\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  alpha=0.01, learning_rate=optimal, loss=perceptron, penalty=l2, score=0.8923215356928614, total=   2.9s\n",
      "[CV] alpha=0.01, learning_rate=optimal, loss=perceptron, penalty=l2 ..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thiagoPanini\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  alpha=0.01, learning_rate=optimal, loss=perceptron, penalty=l2, score=0.895744787239362, total=   3.0s\n",
      "[CV] alpha=0.01, learning_rate=optimal, loss=perceptron, penalty=l2 ..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thiagoPanini\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  alpha=0.01, learning_rate=optimal, loss=perceptron, penalty=l2, score=0.8937340601090163, total=   3.0s\n",
      "[CV] alpha=0.01, learning_rate=optimal, loss=log, penalty=l1 .........\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thiagoPanini\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  alpha=0.01, learning_rate=optimal, loss=log, penalty=l1, score=0.8489802039592081, total=  12.3s\n",
      "[CV] alpha=0.01, learning_rate=optimal, loss=log, penalty=l1 .........\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thiagoPanini\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  alpha=0.01, learning_rate=optimal, loss=log, penalty=l1, score=0.8515425771288564, total=  12.4s\n",
      "[CV] alpha=0.01, learning_rate=optimal, loss=log, penalty=l1 .........\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thiagoPanini\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  alpha=0.01, learning_rate=optimal, loss=log, penalty=l1, score=0.8466770015502325, total=  12.4s\n",
      "[CV] alpha=0.01, learning_rate=optimal, loss=log, penalty=l2 .........\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thiagoPanini\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  alpha=0.01, learning_rate=optimal, loss=log, penalty=l2, score=0.8916716656668666, total=   5.0s\n",
      "[CV] alpha=0.01, learning_rate=optimal, loss=log, penalty=l2 .........\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thiagoPanini\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  alpha=0.01, learning_rate=optimal, loss=log, penalty=l2, score=0.8960948047402371, total=   4.9s\n",
      "[CV] alpha=0.01, learning_rate=optimal, loss=log, penalty=l2 .........\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thiagoPanini\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  alpha=0.01, learning_rate=optimal, loss=log, penalty=l2, score=0.8927339100865129, total=   5.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 180 out of 180 | elapsed: 19.0min finished\n",
      "C:\\Users\\thiagoPanini\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score='raise',\n",
       "       estimator=SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\n",
       "       eta0=0.0, fit_intercept=True, l1_ratio=0.15,\n",
       "       learning_rate='optimal', loss='hinge', max_iter=None, n_iter=None,\n",
       "       n_jobs=1, penalty='l2', power_t=0.5, random_state=None,\n",
       "       shuffle=True, tol=None, verbose=0, warm_start=False),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid=[{'loss': ['hinge', 'perceptron', 'log'], 'penalty': ['l1', 'l2'], 'alpha': array([0.0001, 0.0012, 0.0023, 0.0034, 0.0045, 0.0056, 0.0067, 0.0078,\n",
       "       0.0089, 0.01  ]), 'learning_rate': ['optimal']}],\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring='accuracy', verbose=4)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Aplicando GridSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Definindo parâmetros para Decision Trees\n",
    "param_grid_sgd = [\n",
    "    {\n",
    "        'loss': ['hinge', 'perceptron', 'log'],\n",
    "        'penalty': ['l1', 'l2'],\n",
    "        'alpha': np.linspace(0.0001, 0.01, 10),\n",
    "        'learning_rate': ['optimal']\n",
    "    }\n",
    "    \n",
    "]\n",
    "\n",
    "# Criando classificador\n",
    "sgd_clf = SGDClassifier()\n",
    "\n",
    "# Treinando e procurando a melhor combinação\n",
    "grid_search = GridSearchCV(sgd_clf, param_grid_sgd, cv=3, \n",
    "                           scoring='accuracy', verbose=4)\n",
    "grid_search.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-07T00:05:06.460453Z",
     "start_time": "2019-01-07T00:05:06.454471Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'alpha': 0.0001, 'learning_rate': 'optimal', 'loss': 'hinge', 'penalty': 'l2'}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Melhores hiperparâmetros\n",
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-07T00:05:24.891966Z",
     "start_time": "2019-01-07T00:05:24.885987Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9096666666666666"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Melhor score\n",
    "grid_search.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-07T01:21:04.286410Z",
     "start_time": "2019-01-07T01:21:04.280812Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'acc_train': 0.8825, 'acc_train_cv': 0.8678, 'acc_train_scaled': 0.9101}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verificando scores até o momento\n",
    "dict_accs_sgd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A acurácia (nos dados de treino) obtida através da aplicação do ```GridSearchCV``` não sofreu alterações com relação ao já obtido através da padronização dos dados. Provavalmente, maiores efeitos poderão ser visualizados com a avaliação nos dados de teste."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dados de Teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-07T01:23:57.103109Z",
     "start_time": "2019-01-07T01:23:50.252452Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thiagoPanini\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia obtida em SGD Classifier com os dados de teste: 0.8841\n"
     ]
    }
   ],
   "source": [
    "# Acurácia sem tratamento\n",
    "sgd_clf.fit(X_train, y_train)\n",
    "test_pred = sgd_clf.predict(X_train)\n",
    "acc_test = accuracy_score(y_train, test_pred)\n",
    "\n",
    "# Comunicando resultados\n",
    "print(f'Acurácia obtida em SGD Classifier com os dados de teste: {acc_test:.4f}')\n",
    "\n",
    "# Salvando resultados\n",
    "dict_accs_sgd['acc_test'] = round(acc_test, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Valor relativamente próximo ao obtido com os dados de treino. Sem perdas significativas. Isto abre uma visão otimista sobre as melhorias possíveis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-07T01:26:43.310186Z",
     "start_time": "2019-01-07T01:26:34.800079Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thiagoPanini\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:475: DataConversionWarning: Data with input dtype uint8 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\thiagoPanini\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia nos dados de teste após padronização: 0.9139\n"
     ]
    }
   ],
   "source": [
    "# Treinando modelo com padronização\n",
    "X_train_scaled, X_test_scaled = data_scaled(X_train, X_test)\n",
    "\n",
    "sgd_clf.fit(X_train_scaled, y_train)\n",
    "pred_scaled = sgd_clf.predict(X_test_scaled)\n",
    "\n",
    "acc_scaled = accuracy_score(y_test, pred_scaled)\n",
    "\n",
    "# Comunicando e salvando resultados\n",
    "print(f'Acurácia nos dados de teste após padronização: {acc_scaled:.4f}')\n",
    "dict_accs_sgd['acc_test_scaled'] = round(acc_scaled, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Semelhante ao obtido com os dados de treino. Vamos avaliar agora a performance obtida com os hiperparâmetros retornados pela busca com ```GridSearchCV``` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-07T01:29:12.136859Z",
     "start_time": "2019-01-07T01:29:06.227703Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thiagoPanini\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia final (após GridSearchCV): 0.9139\n"
     ]
    }
   ],
   "source": [
    "# Avaliando resultado com os melhores parâmetros\n",
    "model = grid_search.best_estimator_\n",
    "\n",
    "model.fit(X_train_scaled, y_train)\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "acc_final = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f'Acurácia final (após GridSearchCV): {acc_final:.4f}')\n",
    "\n",
    "# Salvando resultados\n",
    "dict_accs_sgd['acc_test_grid'] = round(acc_final, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exatamente idêntico ao resultado anterior. Vamos verificar a configuração de cada um dos modelos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-07T01:30:41.875830Z",
     "start_time": "2019-01-07T01:30:41.870546Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method BaseEstimator.get_params of SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\n",
       "       eta0=0.0, fit_intercept=True, l1_ratio=0.15,\n",
       "       learning_rate='optimal', loss='hinge', max_iter=None, n_iter=None,\n",
       "       n_jobs=1, penalty='l2', power_t=0.5, random_state=None,\n",
       "       shuffle=True, tol=None, verbose=0, warm_start=False)>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Modelo sgd_clf\n",
    "sgd_clf.get_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-07T01:30:51.552500Z",
     "start_time": "2019-01-07T01:30:51.546503Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method BaseEstimator.get_params of SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\n",
       "       eta0=0.0, fit_intercept=True, l1_ratio=0.15,\n",
       "       learning_rate='optimal', loss='hinge', max_iter=None, n_iter=None,\n",
       "       n_jobs=1, penalty='l2', power_t=0.5, random_state=None,\n",
       "       shuffle=True, tol=None, verbose=0, warm_start=False)>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Modelo grid_search\n",
    "model.get_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A configuração obtida com o grid search foi exatamente idêntica a configuração padrão, o que explica os resultados obtidos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shifting Images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mais uma vez, vamos aplicar a técnica de deslocamento de imagens para avantajar o conteúdo do dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-07T01:34:03.053570Z",
     "start_time": "2019-01-07T01:33:18.106743Z"
    }
   },
   "outputs": [],
   "source": [
    "# Alterando dataset\n",
    "X_train_augmented, y_train_augmented = augment_data(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-07T01:34:03.069525Z",
     "start_time": "2019-01-07T01:34:03.057557Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300000, 784)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verificando dimensões\n",
    "X_train_augmented.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-07T01:34:38.187948Z",
     "start_time": "2019-01-07T01:34:37.757070Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x20800d22860>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotando digitos\n",
    "plot_data_augmented(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-07T01:39:24.389538Z",
     "start_time": "2019-01-07T01:38:51.208253Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thiagoPanini\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia final após o deslocamento de pixels: 0.8532\n"
     ]
    }
   ],
   "source": [
    "# Treinando com o melhor modelo\n",
    "sgd_final = grid_search.best_estimator_\n",
    "sgd_final.fit(X_train_augmented, y_train_augmented)\n",
    "\n",
    "# Realizando predições\n",
    "y_pred = sgd_final.predict(X_test)\n",
    "final_acc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Comunicando resultados\n",
    "print(f'Acurácia final após o deslocamento de pixels: {final_acc:.4f}')\n",
    "\n",
    "# Salvando resultados\n",
    "dict_accs_sgd['acc_test_shifted'] = round(final_acc, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Após a análise dos resultados, alguns levantamentos podem ser feitos:\n",
    "    - SGD Classifier não é suscetível a overfitting (cross validation provou isso);\n",
    "    - A padronização dos dados auxiliou no aumento da performance do modelo;\n",
    "    - O hyperparameter tuning retornou uma combinação muito próximo do classificador padrão;\n",
    "    - Não houve melhoria através do processo data augmentation (pelo contrário, houve piora)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-07T01:47:51.894536Z",
     "start_time": "2019-01-07T01:47:51.750267Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acc_train</th>\n",
       "      <th>acc_train_cv</th>\n",
       "      <th>acc_train_scaled</th>\n",
       "      <th>acc_test</th>\n",
       "      <th>acc_test_scaled</th>\n",
       "      <th>acc_test_grid</th>\n",
       "      <th>acc_test_shifted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.8617</td>\n",
       "      <td>0.8604</td>\n",
       "      <td>0.8800</td>\n",
       "      <td>0.6226</td>\n",
       "      <td>0.8890</td>\n",
       "      <td>0.9211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.8825</td>\n",
       "      <td>0.8678</td>\n",
       "      <td>0.9101</td>\n",
       "      <td>0.8841</td>\n",
       "      <td>0.9139</td>\n",
       "      <td>0.9139</td>\n",
       "      <td>0.8532</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   acc_train  acc_train_cv  acc_train_scaled  acc_test  acc_test_scaled  \\\n",
       "0     1.0000        0.8617            0.8604    0.8800           0.6226   \n",
       "1     0.8825        0.8678            0.9101    0.8841           0.9139   \n",
       "\n",
       "   acc_test_grid  acc_test_shifted  \n",
       "0         0.8890            0.9211  \n",
       "1         0.9139            0.8532  "
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Transformando dados em dataset e salvando\n",
    "dataset_accs = dataset_accs.append(dict_accs_sgd, ignore_index=True)\n",
    "\n",
    "save_dataset(dataset_accs)\n",
    "dataset_accs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As Florestas Aleatórias \"é\" um algoritmo famoso e muito utilizado em diversos problemas. [Doc](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-07T01:49:50.907231Z",
     "start_time": "2019-01-07T01:49:49.714089Z"
    }
   },
   "outputs": [],
   "source": [
    "# Preparando dados\n",
    "X_train, y_train, X_test, y_test = prepare_mnist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-07T01:50:01.962391Z",
     "start_time": "2019-01-07T01:50:01.853814Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x20801398588>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualizando dígitos\n",
    "plot_mnist(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-07T01:50:25.003132Z",
     "start_time": "2019-01-07T01:50:24.982215Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acc_train</th>\n",
       "      <th>acc_train_cv</th>\n",
       "      <th>acc_train_scaled</th>\n",
       "      <th>acc_test</th>\n",
       "      <th>acc_test_scaled</th>\n",
       "      <th>acc_test_grid</th>\n",
       "      <th>acc_test_shifted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.8617</td>\n",
       "      <td>0.8604</td>\n",
       "      <td>0.8800</td>\n",
       "      <td>0.6226</td>\n",
       "      <td>0.8890</td>\n",
       "      <td>0.9211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.8825</td>\n",
       "      <td>0.8678</td>\n",
       "      <td>0.9101</td>\n",
       "      <td>0.8841</td>\n",
       "      <td>0.9139</td>\n",
       "      <td>0.9139</td>\n",
       "      <td>0.8532</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   acc_train  acc_train_cv  acc_train_scaled  acc_test  acc_test_scaled  \\\n",
       "0     1.0000        0.8617            0.8604    0.8800           0.6226   \n",
       "1     0.8825        0.8678            0.9101    0.8841           0.9139   \n",
       "\n",
       "   acc_test_grid  acc_test_shifted  \n",
       "0         0.8890            0.9211  \n",
       "1         0.9139            0.8532  "
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lendo dataset de resultados até o momento\n",
    "dataset_accs = load_dataset()\n",
    "dataset_accs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lembrando que:\n",
    "    - 0: Decision Trees\n",
    "    - 1: SGD Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-07T01:51:21.409181Z",
     "start_time": "2019-01-07T01:51:21.405191Z"
    }
   },
   "outputs": [],
   "source": [
    "# Criando dicionario vazio para armazenar performances\n",
    "dict_accs_forest = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dados de Treino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-07T01:53:18.821901Z",
     "start_time": "2019-01-07T01:53:11.404731Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia do modelo Random Forest com os dados de treino: 0.9994\n"
     ]
    }
   ],
   "source": [
    "# Criando classificador\n",
    "forest = RandomForestClassifier()\n",
    "\n",
    "# Treinando modelo e avaliando performance\n",
    "forest.fit(X_train, y_train)\n",
    "train_pred = forest.predict(X_train)\n",
    "train_acc = accuracy_score(y_train, train_pred)\n",
    "\n",
    "# Comunicando resultados\n",
    "print(f'Acurácia do modelo Random Forest com os dados de treino: {train_acc:.4f}')\n",
    "\n",
    "# Salvando resultados\n",
    "dict_accs_forest['acc_train'] = round(train_acc, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como o modelo ```Decision Trees``` é extremamente suscetível a overfitting, também o modelo ```Random Forest``` pode ser. A própria acurácia obtida nos dados de treinamento é um grande indício disso. Mais uma vez, para verificar tal teste, será usado o ```cross validation```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-07T01:55:43.057462Z",
     "start_time": "2019-01-07T01:55:29.705185Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores: [0.94121176 0.94054703 0.94184128]\n",
      "Média: 0.9412\n",
      "Desvio Padrão: 0.0005\n"
     ]
    }
   ],
   "source": [
    "# Aplicando validação cruzada\n",
    "forest_scores = cross_val_score(forest, X_train, y_train,\n",
    "                               cv=3, scoring='accuracy')\n",
    "display_scores(forest_scores)\n",
    "\n",
    "# Salvando resultados\n",
    "dict_accs_forest['acc_train_cv'] = round(forest_scores.mean(), 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A queda da acurácia com a validação cruzada comprova que ```Random Forest``` tem tendências ao overfitting. Porém, com a queda menos acentuada que no modelo ```Decision Trees``` é possível prever que os resultados finais serão mais animadores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tunando Hiperparâmetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
